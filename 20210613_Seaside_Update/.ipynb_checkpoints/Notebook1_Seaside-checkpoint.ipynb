{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN-CORE Flowchart - Seaside\n",
    "This notebook uses the Seaside testbed to demonstrate the following components of the IN-CORE flowchart. \n",
    "+ [Start](#0)\n",
    "+ [1. Initial Community Description](#1)\n",
    "  + [1a. Built Environment](#1a)\n",
    "  + [1b. Social Systems](#1b)\n",
    "  + [1c. Economic Systems](#1c)\n",
    "+ [2. Hazards and Damages](#2)\n",
    "  + [2a. Hazard Models](#2a)\n",
    "  + [2b. Damage Models](#2b)\n",
    "  + [2c. Damage to Physical Infrastructure](#2c)\n",
    "+ [3. Functionality](#3)\n",
    "  + [3a. Functionality Models](#3a)\n",
    "  + [3b. Functionality of Physical Infrastructure](#3b)\n",
    "  + [3c. CGE Model](#3c)\n",
    "  + [3d. Social Science Modules](#3d)\n",
    "  + [3e. Direct and Indirect Economic & Social Losses](#3e)\n",
    "+ [4. Recovery](#4)\n",
    "+ [References](#ref)\n",
    "\n",
    "\n",
    "<!-- *Notebook created by Dylan R. Sanderson (OSU - sanderdy@oregonstate.edu) -->\n",
    "\n",
    "![title](images/Flowchart_0.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Background\n",
    "\n",
    "The North American Pacific Northwest is subject to the rupture of the Cascadia Subduction Zone (CSZ), which is an approximately 1,000 km long fault located between Cape Mendocino California and Vancouver Island, Canada, and separates the Juan de Fuca and North America plates. Rupture of the CSZ can result in both strong earthquake ground shaking and tsunami inundation. The last full rupture of the CSZ occurred in 1700 and is estimated to have had a moment magnitude between 8.7 and 9.2. The city of Seaside is a small coastal town located along the northern Oregon coast, and has a full-time population of approximately 6,700 people. As a popular coastal town, Seaside seeâ€™s large population flucuations both seasonally (e.g. winter vs. summer population) and weekly (e.g. weekday vs. weekend population).\n",
    "\n",
    "Here, Seaside is employed as a testbed community to demonstrate how IN-CORE can be used for communities subject to multiple hazards. The small size of Seaside naturally enables a fine level of analysis leading to a high level of detail. The analysis in this notebook are at the parcel level, which is defined as the size of a tax-lot and roughly corresponds to a single building.\n",
    "\n",
    "\n",
    "\n",
    "![title](images/seaside.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "***\n",
    "## Start\n",
    "**Importing all of the pyIncore modules used for this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyincore import IncoreClient, Dataset, DataService, HazardService, FragilityService\n",
    "from pyincore import FragilityCurveSet, MappingSet\n",
    "from pyincore_viz.geoutil import GeoUtil as geoviz\n",
    "from pyincore_viz.plotutil import PlotUtil as plotviz\n",
    "\n",
    "from pyincore.analyses.housingunitallocation import HousingUnitAllocation\n",
    "from pyincore.analyses.populationdislocation import PopulationDislocation, PopulationDislocationUtil\n",
    "from pyincore.analyses.buildingdamage import BuildingDamage\n",
    "from pyincore.analyses.buildingfunctionality import BuildingFunctionality\n",
    "from pyincore.analyses.montecarlofailureprobability import MonteCarloFailureProbability\n",
    "from pyincore.analyses.cumulativebuildingdamage import CumulativeBuildingDamage\n",
    "from pyincore.analyses.epfdamage import EpfDamage\n",
    "from pyincore.analyses.bridgedamage import BridgeDamage\n",
    "from pyincore.analyses.roaddamage import RoadDamage\n",
    "from pyincore.analyses.pipelinedamage import PipelineDamage\n",
    "from pyincore.analyses.pipelinedamagerepairrate import PipelineDamageRepairRate\n",
    "from pyincore.analyses.waterfacilitydamage import WaterFacilityDamage\n",
    "from pyincore.analyses.capitalshocks import CapitalShocks\n",
    "from pyincore.analyses.seasidecge import SeasideCGEModel\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import contextily as ctx\n",
    "\n",
    "\n",
    "client = IncoreClient()\n",
    "data_service = DataService(client)\n",
    "hazard_service = HazardService(client)\n",
    "fragility_services = FragilityService(client)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining recurrence interval to consider and number of MC simulations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_prd = int(input('Hazard Recurrence Interval: '))\n",
    "n_sims = int(input('Number of MC Simulations: '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining output directory and creating it if does not exist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_output = os.path.join(os.getcwd(), 'output', '{}yr' .format(ret_prd))\n",
    "if not os.path.exists(path_to_output):\n",
    "    os.makedirs(path_to_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "***\n",
    "## 1) Initial Community Description\n",
    "\n",
    "![title](images/Flowchart_1b.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1a'></a>\n",
    "### 1a) Built Environment\n",
    "\n",
    "The Seaside testbed consists of four infrastructure systems (buildings, electric, transportation, and water). Each infrastructure system may be composed of different infrastructure components. For example, the electric power network consists of electric lines and poles. \n",
    "\n",
    "The infrastructure systems and components are shown below along with their IN-CORE GUID. \n",
    "\n",
    "| Infrastructure System | Infrastructure Component | GUID |\n",
    "| --- | --- | --- |\n",
    "| Buildings | - | 613ba5ef5d3b1d6461e8c415 | \n",
    "| Electric | Electric Poles | 5d263f08b9219cf93c056c68 | \n",
    "| - | Electric Lines | 60e5e326544e944c3ce37a93 | \n",
    "| Transportation | Roads | 60e5e5cd544e944c3ce37d08 | \n",
    "| - | Bridges | 60e5e576d3c92a78c893ff69 | \n",
    "| Water | Water Pumping Stations | 60e5e91960b3f41243faa3b2 | \n",
    "| - | Water Treatment Plant | 60e5e91960b3f41243faa3b2 |\n",
    "| - | Water Pipes | 60e72f9fd3c92a78c89636c7 | \n",
    "\n",
    "\n",
    "This section:\n",
    "+ Reads in Seaside infrastructure systems from the pyIncore dataservice as a dataset object\n",
    "+ Uses pyincore_viz to plot infrastructure components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buildings\n",
    "The building inventory for Seaside was originally collected by Park *et al.* (2017a) using a combination of tax-lot assessor data, field data collection, and Google street view. The terms parcels and buildings are used interchangeably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bldg_dataset_id = \"613ba5ef5d3b1d6461e8c415\"        # defining building dataset (GIS point layer)\n",
    "bldg_dataset = Dataset.from_data_service(bldg_dataset_id, data_service)\n",
    "geoviz.plot_map(bldg_dataset, column='struct_typ',category='True')\n",
    "bldg_df = bldg_dataset.get_dataframe_from_shapefile()\n",
    "bldg_df.set_index('guid', inplace=True)\n",
    "print('Number of buildings: {}' .format(len(bldg_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Electric\n",
    "The electric infrastructure was originallly collected by Kameshwar *et al.* (2019) and consists of electric poles, an electric substation, and electric power lines. Each building parcel in Seaside is mapped to the nearest electric pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_pole_dataset_id = \"5d263f08b9219cf93c056c68\"        # defining electric pole dataset (GIS point layer)\n",
    "elec_pole_dataset = Dataset.from_data_service(elec_pole_dataset_id, data_service)\n",
    "geoviz.plot_map(elec_pole_dataset, column=None, category=False)\n",
    "\n",
    "\n",
    "elec_line_dataset_id = \"60e5e326544e944c3ce37a93\"        # defining electric line dataset (GIS point layer)\n",
    "elec_line_dataset = Dataset.from_data_service(elec_line_dataset_id, data_service)\n",
    "geoviz.plot_map(elec_line_dataset, column=None, category=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transportation\n",
    "The transportation infrastructure was originallly collected by Kameshwar *et al.* (2019) and consists of roads and bridges. Each parcel in Seaside is mapped to the nearest node in the transportation network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trns_road_dataset_id = \"60e5e5cd544e944c3ce37d08\"        # defining transportation road dataset (GIS point layer)\n",
    "trns_road_dataset = Dataset.from_data_service(trns_road_dataset_id, data_service)\n",
    "geoviz.plot_map(trns_road_dataset, column=None, category=False)\n",
    "\n",
    "\n",
    "trns_brdg_dataset_id = \"60e5e576d3c92a78c893ff69\"        # defining transportation bridge dataset (GIS point layer)\n",
    "trns_brdg_dataset = Dataset.from_data_service(trns_brdg_dataset_id, data_service)\n",
    "geoviz.plot_map(trns_brdg_dataset, column=None, category=False, basemap=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Water\n",
    "The water infrastructure was originallly collected by Kameshwar *et al.* (2019) and consists of water pipes, a water treatment plant, and three water pumping stations. Each building parcel in Seaside is mapped to the nearest node in the water network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wter_pipe_dataset_id = \"60e72f9fd3c92a78c89636c7\"        # defining water pipes (GIS point layer)\n",
    "wter_pipe_dataset = Dataset.from_data_service(wter_pipe_dataset_id, data_service)\n",
    "geoviz.plot_map(wter_pipe_dataset, column=None, category=False)\n",
    "\n",
    "wter_fclty_dataset_id = \"60e5e91960b3f41243faa3b2\"        # defining water facilities (GIS point layer)\n",
    "wter_fclty_dataset = Dataset.from_data_service(wter_fclty_dataset_id, data_service)\n",
    "geoviz.plot_map(wter_fclty_dataset, column='utilfcltyc', category=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1b'></a>\n",
    "### 1b) Social Systems\n",
    "\n",
    "Seaside has a permanent resident population of approximately 6,700 people.\n",
    "\n",
    "This section performs a housing unit allocation. The housing unit inventory includes characteristics for individual households and housing units that can be linked to residential buildings. \n",
    "For more information see Rosenheim et al. (2019)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create housing allocation \n",
    "hua = HousingUnitAllocation(client)\n",
    "\n",
    "# Load input dataset\n",
    "housing_unit_inv_id = \"5d543087b9219c0689b98234\"\n",
    "address_point_inv_id = \"5d542fefb9219c0689b981fb\"\n",
    "bldg_inv_id = \"613ba5ef5d3b1d6461e8c415\" \n",
    "\n",
    "hua.load_remote_input_dataset(\"housing_unit_inventory\", housing_unit_inv_id)\n",
    "hua.load_remote_input_dataset(\"address_point_inventory\", address_point_inv_id)\n",
    "hua.load_remote_input_dataset(\"buildings\", bldg_inv_id) \n",
    "\n",
    "\n",
    "# Set analysis parameters\n",
    "path_out = os.path.join(path_to_output, 'hua_0')\n",
    "hua.set_parameter(\"result_name\", path_out)\n",
    "hua.set_parameter(\"seed\", 1337)\n",
    "hua.set_parameter(\"iterations\", 1)\n",
    "hua.run_analysis()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore results from Housing Unit Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve result dataset\n",
    "hua_result = hua.get_output_dataset(\"result\")\n",
    "\n",
    "# Convert dataset to Pandas DataFrame\n",
    "hua_df = hua_result.get_dataframe_from_csv(low_memory=False)\n",
    "\n",
    "# Display top 5 rows of output data\n",
    "print(hua_df[['guid','numprec','ownershp','geometry','aphumerge']].head())\n",
    "\n",
    "# keep observations where the housing unit characteristics have been allocated to a structure.\n",
    "hua_df = hua_df.dropna(subset=['guid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hua_df['Race Ethnicity'] = \"0 Vacant HU No Race Ethnicity Data\"\n",
    "hua_df['Race Ethnicity'].notes = \"Identify Race and Ethnicity Housing Unit Characteristics.\"\n",
    "\n",
    "hua_df.loc[(hua_df['race'] == 1) & \n",
    "                        (hua_df['hispan'] == 0),'Race Ethnicity'] = \"1 White alone, Not Hispanic\"\n",
    "hua_df.loc[(hua_df['race'] == 2) & \n",
    "                        (hua_df['hispan'] == 0),'Race Ethnicity'] = \"2 Black alone, Not Hispanic\"\n",
    "hua_df.loc[(hua_df['race'].isin([3,4,5,6,7])) & \n",
    "                        (hua_df['hispan'] == 0),'Race Ethnicity'] = \"3 Other Race, Not Hispanic\"\n",
    "hua_df.loc[(hua_df['hispan'] == 1),'Race Ethnicity'] = \"4 Any Race, Hispanic\"\n",
    "hua_df.loc[(hua_df['gqtype'] >= 1),'Race Ethnicity'] = \"5 Group Quarters no Race Ethnicity Data\"\n",
    "\n",
    "hua_df['Tenure Status'] = \"0 No Tenure Status\"\n",
    "hua_df.loc[(hua_df['ownershp'] == 1),'Tenure Status'] = \"1 Owner Occupied\"\n",
    "hua_df.loc[(hua_df['ownershp'] == 2),'Tenure Status'] = \"2 Renter Occupied\"\n",
    "hua_df['Tenure Status'].notes = \"Identify Tenure Status Housing Unit Characteristics.\"\n",
    "\n",
    "hua_df.to_csv(os.path.join(path_to_output, 'hua_df.csv'))\n",
    "\n",
    "# make pivot table\n",
    "table = pd.pivot_table(hua_df, values='numprec', index=['Race Ethnicity'],\n",
    "                                     margins = True, margins_name = 'Total',\n",
    "                                     columns=['Tenure Status'], aggfunc=[np.sum]).rename(\n",
    "    columns={'Total': 'Total Population', 'sum': ''})\n",
    "table_title = \"Table 1. Total Population by Race, Ethncity, and Tenure Status, Seaside, OR, 2010.\"\n",
    "varformat = {('','Total Population'): \"{:,.0f}\",\n",
    "             ('','0 No Tenure Status'): \"{:,.0f}\",\n",
    "             ('','1 Owner Occupied'): \"{:,.0f}\",\n",
    "             ('','2 Renter Occupied'): \"{:,.0f}\"}\n",
    "table.style.set_caption(table_title).format(varformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use shapely.wkt loads to convert WKT to GeoSeries\n",
    "from shapely.geometry import Point\n",
    "# Geodata frame requires geometry and CRS to be set\n",
    "hua_gdf = gpd.GeoDataFrame(\n",
    "    hua_df,\n",
    "    crs={'init': 'epsg:4326'},\n",
    "    geometry=[Point(xy) for xy in zip(hua_df['x'], hua_df['y'])])\n",
    "hua_gdf[['guid','x','y','ownershp','geometry']].head(6)\n",
    "\n",
    "# visualize population\n",
    "gdf = hua_gdf\n",
    "geoviz.plot_gdf_map(gdf, column='ownershp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1c'></a>\n",
    "### 1c) Economic Systems\n",
    "\n",
    "For the CGE model, each parcel is mapped to an economic sector. The sector of each parcel is plotted spatially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bldg2sector_id = \"5f5fd8e2980a301080a03996\"\n",
    "bldg2sector_ds = Dataset.from_data_service(bldg2sector_id, data_service)\n",
    "bldg2sector_df = bldg2sector_ds.get_dataframe_from_csv()\n",
    "\n",
    "\n",
    "df = pd.merge(gdf, bldg2sector_df, left_on='guid', right_on='guid')\n",
    "geoviz.plot_gdf_map(df, column='sector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "***\n",
    "## 2) Hazards and Damages\n",
    "\n",
    "![title](images/Flowchart_2.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2a'></a>\n",
    "### 2a) Hazard Models\n",
    "\n",
    "The Seaside testbed uses hazard layers that were developed outside of IN-CORE. The hazard layers are the result of a Probabilistic Seismic and Tsunami Hazard Analysis (PSTHA; Park *et al.,* 2017b). The hazard layers and GUID's are in the table below\n",
    "\n",
    "| Recurrence Interval | Earthquake | Tsunami |\n",
    "| --- | --- | --- |\n",
    "| 100-yr | 5dfa4058b9219c934b64d495 | 5bc9e25ef7b08533c7e610dc |\n",
    "| 250-yr | 5dfa41aab9219c934b64d4b2 | 5df910abb9219cd00cf5f0a5 |\n",
    "| 500-yr | 5dfa4300b9219c934b64d4d0 | 5df90e07b9219cd00ce971e7 |\n",
    "| 1,000-yr | 5dfa3e36b9219c934b64c231 | 5df90137b9219cd00cb774ec |\n",
    "| 2,500-yr | 5dfa4417b9219c934b64d4d3 | 5df90761b9219cd00ccff258 |\n",
    "| 5,000-yr | 5dfbca0cb9219c101fd8a58d | 5df90871b9219cd00ccff273 |\n",
    "| 10,000-yr | 5dfa51bfb9219c934b68e6c2 | 5d27b986b9219c3c55ad37d0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_hazard_dict = {100: \"5dfa4058b9219c934b64d495\", \n",
    "                  250: \"5dfa41aab9219c934b64d4b2\",\n",
    "                  500: \"5dfa4300b9219c934b64d4d0\",\n",
    "                  1000: \"5dfa3e36b9219c934b64c231\",\n",
    "                  2500: \"5dfa4417b9219c934b64d4d3\", \n",
    "                  5000: \"5dfbca0cb9219c101fd8a58d\",\n",
    "                 10000: \"5dfa51bfb9219c934b68e6c2\"}\n",
    "\n",
    "tsu_hazard_dict = {100: \"5bc9e25ef7b08533c7e610dc\", \n",
    "                  250: \"5df910abb9219cd00cf5f0a5\",\n",
    "                  500: \"5df90e07b9219cd00ce971e7\",\n",
    "                  1000: \"5df90137b9219cd00cb774ec\",\n",
    "                  2500: \"5df90761b9219cd00ccff258\",\n",
    "                  5000: \"5df90871b9219cd00ccff273\",\n",
    "                  10000: \"5d27b986b9219c3c55ad37d0\"}\n",
    "\n",
    "eq_demand = 'pga'\n",
    "tsu_demand = 'Vmax'\n",
    "\n",
    "eq_json = hazard_service.get_earthquake_hazard_metadata(hazard_id=eq_hazard_dict[ret_prd])\n",
    "tsu_json = hazard_service.get_tsunami_hazard_metadata(hazard_id=tsu_hazard_dict[ret_prd])\n",
    "\n",
    "for i in range(len(eq_json['hazardDatasets'])):\n",
    "    if eq_json['hazardDatasets'][i]['demandType'].lower() == eq_demand.lower():\n",
    "        eq_dataset = eq_json['hazardDatasets'][i]['datasetId']\n",
    "\n",
    "for i in range(len(tsu_json['hazardDatasets'])):\n",
    "    if tsu_json['hazardDatasets'][i]['demandType'].lower() == tsu_demand.lower():\n",
    "        tsu_dataset = tsu_json['hazardDatasets'][i]['datasetId']\n",
    "\n",
    "# geoviz.plot_raster_dataset(eq_json['hazardDatasets'][1]['datasetId'], client)\n",
    "geoviz.plot_raster_dataset(tsu_json['hazardDatasets'][1]['datasetId'], client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2b'></a>\n",
    "### 2b) Damage Models\n",
    "\n",
    "Seaside damage models are fragility curves from HAZUS. \n",
    "\n",
    "The fragility curve mapping ID's for Seaside are in the table below:\n",
    "\n",
    "| Infrastructure System | Hazard | Mapping GUID |\n",
    "| --- | --- | --- |\n",
    "| Buildings | Earthquake | 5d2789dbb9219c3c553c7977 | \n",
    "| Buildings | Tsunami | 5d279bb9b9219c3c553c7fba | \n",
    "| Electric Poles | Earthquake | 6079f6a7ef881f48a4ace306 |\n",
    "| Electric Poles | Tsunami | 6079c5e06799d908861f177c |\n",
    "| Roads | Earthquake | 6079ec1aef881f48a4ac586c | \n",
    "| Roads | Tsunami | 6079c3bc6799d908861f16d4 | \n",
    "| Bridges | Earthquake | 6079f5435b609c56229bf85c | \n",
    "| Bridges | Tsunami | 6079c41a6799d908861f16fe | \n",
    "| Water Facilities | Earthquake | 5d39e010b9219cc18bd0b0b6 | \n",
    "| Water Facilities | Tsunami | 5d31f737b9219c6d66398521 | \n",
    "| Water Pipes | Earthquake | 5b47c227337d4a38464efea8 |\n",
    "| Water Pipes | Tsunami | 5d320a87b9219c6d66398b45 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting some example fragility curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO: issues with plotting some of these mappings. Looks like it \n",
    "    needs a \"vmax\" value, but unsure where \"\"\"\n",
    "\n",
    "mapping_dict = {\n",
    "    'buildings': \n",
    "        {'eq': '5d2789dbb9219c3c553c7977',\n",
    "         'tsu': '5d279bb9b9219c3c553c7fba'},\n",
    "    \n",
    "    'electric_poles': \n",
    "        {'eq': '6079f6a7ef881f48a4ace306',\n",
    "         'tsu': '6079c5e06799d908861f177c'}, # doesn't plot\n",
    "\n",
    "    'roads': \n",
    "        {'eq': '6079ec1aef881f48a4ac586c',\n",
    "        'tsu': '6079c3bc6799d908861f16d4'},  # doesn't plot\n",
    "    \n",
    "    'bridges': \n",
    "        {'eq': '6079f5435b609c56229bf85c',\n",
    "         'tsu': '6079c41a6799d908861f16fe'},  # doesn't plot\n",
    "    \n",
    "    'water_pipe': \n",
    "        {'eq': '5b47c227337d4a38464efea8',\n",
    "         'tsu': '5d320a87b9219c6d66398b45'},\n",
    "    \n",
    "    'water_facility': \n",
    "        {'eq': '6079f7825b609c56229bf976',\n",
    "         'tsu': '6079b8a66799d908861e4bf0'},  # doesn't plot\n",
    "}\n",
    "\n",
    "\n",
    "mapping_id = mapping_dict['buildings']['eq']\n",
    "\n",
    "mapping_set = MappingSet(fragility_services.get_mapping(mapping_id))\n",
    "\n",
    "# plot fragility for the first 3 archetypes using pyincore viz method\n",
    "for mapping in mapping_set.mappings[:3]:\n",
    "    fragility_id = mapping.entry[list(mapping.entry.keys())[0]]\n",
    "    fragility_set = FragilityCurveSet(fragility_services.get_dfr3_set(fragility_id))\n",
    "    plt = plotviz.get_fragility_plot(fragility_set)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a function to read Monte-Carlo results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MC_results(path_to_mc, n_sims):\n",
    "    \"\"\" Reads the building monte carlo results into a pandas dataframe.\n",
    "        The output data from this analysis needs to be reformatted to be read by \"read_csv\"\n",
    "    \"\"\"\n",
    "    replace_dict = {'DS_0': 0,\n",
    "                    'DS_1': 1,\n",
    "                    'DS_2': 2,\n",
    "                    'DS_3': 3,\n",
    "                    'DS_4': 4}\n",
    "    \n",
    "    df = pd.read_csv(path_to_mc)\n",
    "    cols = [i for i in range(n_sims)]\n",
    "    df[cols] = df['sample_damage_states'].str.split(',',expand=True)\n",
    "    df.set_index('guid', inplace=True)    \n",
    "    del df['sample_damage_states']\n",
    "    df = df[cols].replace(replace_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up an alternative plotting function to plot buildings spatially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "def plot_gdf_map(gdf, column, category=False, basemap=True, source=ctx.providers.OpenStreetMap.Mapnik, **kwargs):\n",
    "    \"\"\"\n",
    "    Taken from pyincore-viz. \n",
    "    Not using the pyincore-viz version b/c it's limited on plotting options\n",
    "        - Added **kwargs for more control over the geopandas plotting function\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1, figsize=(10,15))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "    ax = gdf.plot(figsize=(10, 10), \n",
    "                  column=column,\n",
    "                  categorical=category, \n",
    "                  legend=True,\n",
    "                  ax=ax,\n",
    "                  cax=cax,\n",
    "                 **kwargs)\n",
    "    if basemap:\n",
    "        ctx.add_basemap(ax, source=source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a quick way to read pyIncore datasets into either dataframes or geodataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pyincore_df(client, ds_id, index=None):\n",
    "    \"\"\" reading pyincore dataset and loading into dataframe\n",
    "    \"\"\"\n",
    "    ds = Dataset.from_data_service(ds_id, data_service)\n",
    "    df = ds.get_dataframe_from_csv()\n",
    "    if index != None:\n",
    "        df = df.set_index(index)\n",
    "    return df\n",
    "\n",
    "def read_pyincore_gdf(client, ds_id, index=None):\n",
    "    \"\"\" reading pyincore dataset and loading into a geodataframe\n",
    "    \"\"\"\n",
    "    ds = Dataset.from_data_service(ds_id, data_service)\n",
    "    df = ds.get_dataframe_from_shapefile()\n",
    "    if index != None:\n",
    "        df = df.set_index(index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2c'></a>\n",
    "### 2c) Damage to Physical Infrastructure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Damage\n",
    "Running building damage for earthquake, tsunami, and cumulative earthquake+tsunami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Earthquake\n",
    "# initializing building damage and fragility service\n",
    "bldg_dmg = BuildingDamage(client)   \n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# defining building dataset (GIS point layer)\n",
    "bldg_dataset_id = \"613ba5ef5d3b1d6461e8c415\"\n",
    "bldg_dmg.load_remote_input_dataset(\"buildings\", bldg_dataset_id)\n",
    "\n",
    "# specifiying mapping id from fragilites to building types\n",
    "mapping_id = \"5d2789dbb9219c3c553c7977\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "bldg_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "bldg_dmg.set_parameter(\"hazard_type\", \"earthquake\")\n",
    "bldg_dmg.set_parameter(\"num_cpu\", 4)\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'buildings_eq_{}yr' .format(ret_prd))\n",
    "hazard_id = eq_hazard_dict[ret_prd]\n",
    "bldg_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "bldg_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "bldg_dmg.run_analysis()\n",
    "print('Earthquake done.')\n",
    "\n",
    "\n",
    "\n",
    "# --- Tsunami\n",
    "# initializing pyincore building damage and fragility service \n",
    "bldg_dmg = BuildingDamage(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# defining building dataset (GIS point layer)\n",
    "bldg_dataset_id = \"613ba5ef5d3b1d6461e8c415\"\n",
    "bldg_dmg.load_remote_input_dataset(\"buildings\", bldg_dataset_id)\n",
    "\n",
    "# specifiying mapping id from fragilites to building types\n",
    "mapping_id = \"5d279bb9b9219c3c553c7fba\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "bldg_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "bldg_dmg.set_parameter(\"hazard_type\", \"tsunami\")\n",
    "bldg_dmg.set_parameter(\"num_cpu\", 4)\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'buildings_tsu_{}yr' .format(ret_prd))\n",
    "hazard_id = tsu_hazard_dict[ret_prd]\n",
    "bldg_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "bldg_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "bldg_dmg.run_analysis()\n",
    "print('Tsunami done.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Cumulative\n",
    "# initializing pyIncore cumulative building damage\n",
    "cumulative_bldg_dmg = CumulativeBuildingDamage(client)\n",
    "cumulative_bldg_dmg.set_parameter(\"num_cpu\", 4)\n",
    "\n",
    "# reading in damage results from above analysis\n",
    "eq_damage_results_csv = os.path.join(path_to_output, 'buildings_eq_{}yr.csv' .format(ret_prd))\n",
    "tsu_damage_results_csv = os.path.join(path_to_output, 'buildings_tsu_{}yr.csv' .format(ret_prd))\n",
    "\n",
    "# loading datasets from CSV files into pyincore\n",
    "eq_damage_dataset = Dataset.from_file(eq_damage_results_csv, \"ergo:buildingDamageVer5\")\n",
    "tsu_damage_dataset = Dataset.from_file(tsu_damage_results_csv, \"ergo:buildingDamageVer5\")\n",
    "\n",
    "cumulative_bldg_dmg.set_input_dataset(\"eq_bldg_dmg\", eq_damage_dataset)\n",
    "cumulative_bldg_dmg.set_input_dataset(\"tsunami_bldg_dmg\", tsu_damage_dataset)\n",
    "\n",
    "# defining path to output \n",
    "result_name = os.path.join(path_to_output, 'buildings_cumulative_{}yr' .format(ret_prd))\n",
    "cumulative_bldg_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "# running analysis\n",
    "cumulative_bldg_dmg.run_analysis()\n",
    "print('Cumulative done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building MC Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_dmg_result = cumulative_bldg_dmg.get_output_dataset('combined-result')\n",
    "\n",
    "path_to_mc_results = os.path.join(path_to_output, 'mc_buildings_cumulative_{}yr' .format(ret_prd))\n",
    "\n",
    "mc_bldg = MonteCarloFailureProbability(client)\n",
    "mc_bldg.set_input_dataset(\"damage\", building_dmg_result)\n",
    "mc_bldg.set_parameter(\"num_cpu\", 8)\n",
    "mc_bldg.set_parameter(\"num_samples\", n_sims)\n",
    "mc_bldg.set_parameter(\"damage_interval_keys\", [\"DS_0\", \"DS_1\", \"DS_2\", \"DS_3\"])\n",
    "mc_bldg.set_parameter(\"failure_state_keys\", [\"DS_1\", \"DS_2\", \"DS_3\"])\n",
    "\n",
    "mc_bldg.set_parameter(\"result_name\", path_to_mc_results) # name of csv file with results\n",
    "mc_bldg.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore Monte-Carlo Building Damage Results**\n",
    "\n",
    "Each row is a parcel, each column is a sample from the Monte-Carlo analysis. \n",
    "+ 0: No/Insignificant Damage\n",
    "+ 1: Moderate Damage\n",
    "+ 2: Heavy Damage\n",
    "+ 3: Complete Damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_df = os.path.join(path_to_output, 'mc_buildings_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "mc_df = read_MC_results(path_to_df, n_sims)\n",
    "mc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df['avg'] = mc_df.mean(axis=1) # getting average of MC results\n",
    "\n",
    "# reading pyIncore dataset to geodataframe\n",
    "bldg_dataset_id = \"613ba5ef5d3b1d6461e8c415\"        # defining building dataset (GIS point layer)\n",
    "bldg_gdf = read_pyincore_gdf(client, bldg_dataset_id, index='guid')\n",
    "\n",
    "# merging the two above\n",
    "bldg_mc_df = pd.merge(bldg_gdf, mc_df['avg'], left_index=True, right_index=True)\n",
    "\n",
    "# plotting results\n",
    "plot_gdf_map(bldg_mc_df, column='avg', vmin=0, vmax=3, s=4, cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Electric Damage\n",
    "Running electric power network damage for earthquake, tsunami, and cumulative earthquake+tsunami. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- Earthquake \n",
    "# initializing epf damage and fragility service\n",
    "epf_dmg = EpfDamage(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# defining electric pole dataset (GIS point layer)\n",
    "poles_ss_id = \"5d263f08b9219cf93c056c68\"     # elelctric power poles and substation\n",
    "epf_dmg.load_remote_input_dataset(\"epfs\", poles_ss_id)\n",
    "\n",
    "# Fragility Mapping on incore-service\n",
    "mapping_id = \"5d489aa1b9219c0689f1988e\" # 5 DS\n",
    "# mapping_id = \"6079f6a7ef881f48a4ace306\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "epf_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "epf_dmg.set_parameter(\"hazard_type\", 'earthquake')\n",
    "epf_dmg.set_parameter(\"num_cpu\", 4)\n",
    "epf_dmg.set_parameter('fragility_key', \"pga\")\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'electric_eq_{}yr'.format(ret_prd))\n",
    "hazard_id = eq_hazard_dict[ret_prd]\n",
    "\n",
    "epf_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "epf_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "# Run Analysis\n",
    "epf_dmg.run_analysis()\n",
    "print('Earthquake done.')\n",
    "\n",
    "\n",
    "# --- Tsunami\n",
    "# Initializing EPF damage and fragility service\n",
    "epf_dmg = EpfDamage(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# defining electric poles and substation dataset (GIS point layer)\n",
    "poles_ss_id = \"5d263f08b9219cf93c056c68\"\n",
    "epf_dmg.load_remote_input_dataset(\"epfs\", poles_ss_id)\n",
    "\n",
    "# Fragility Mapping on incore-service\n",
    "mapping_id = \"5d31eb7fb9219c6d66398445\" # 5 DS\n",
    "# mapping_id = \"6079c5e06799d908861f177c\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "epf_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "epf_dmg.set_parameter(\"hazard_type\", \"tsunami\")\n",
    "epf_dmg.set_parameter(\"num_cpu\", 4)\n",
    "epf_dmg.set_parameter('fragility_key', \"Non-Retrofit inundationDepth Fragility ID Code\")\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'electric_tsu_{}yr'.format(ret_prd))\n",
    "hazard_id = tsu_hazard_dict[ret_prd]\n",
    "\n",
    "epf_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "epf_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "# Run Analysis\n",
    "epf_dmg.run_analysis()\n",
    "print('Tsunami done.')\n",
    "\n",
    "\n",
    "# --- Cumulative\n",
    "print('\\nCumulative electric damage does not exist in pyIncore yet. Doing manually.')\n",
    "path_to_eq = os.path.join(path_to_output, 'electric_eq_{}yr.csv' .format(ret_prd))\n",
    "path_to_ts = os.path.join(path_to_output, 'electric_tsu_{}yr.csv' .format(ret_prd))\n",
    "\n",
    "eq_df = pd.read_csv(path_to_eq, index_col=0)\n",
    "ts_df = pd.read_csv(path_to_ts, index_col=0)\n",
    "\n",
    "cumulative = pd.DataFrame(index=eq_df.index)\n",
    "cumulative[\"LS_0\"] = eq_df[\"LS_0\"] + ts_df[\"LS_0\"] \\\n",
    "    - eq_df[\"LS_0\"] * ts_df[\"LS_0\"]\n",
    "\n",
    "cumulative[\"LS_1\"] = eq_df[\"LS_1\"] + ts_df[\"LS_1\"] - eq_df[\"LS_1\"] * ts_df[\"LS_1\"] \\\n",
    "    + ((eq_df[\"LS_0\"] - eq_df[\"LS_1\"]) * (ts_df[\"LS_0\"] - ts_df[\"LS_1\"]))\n",
    "\n",
    "cumulative[\"LS_2\"] = eq_df[\"LS_2\"] + ts_df[\"LS_2\"] \\\n",
    "    - eq_df[\"LS_2\"] * ts_df[\"LS_2\"] \\\n",
    "    + ((eq_df[\"LS_1\"] - eq_df[\"LS_2\"]) * (ts_df[\"LS_1\"] - ts_df[\"LS_2\"]))\n",
    "\n",
    "cumulative[\"LS_3\"] = eq_df[\"LS_3\"] + ts_df[\"LS_3\"] \\\n",
    "    - eq_df[\"LS_3\"] * ts_df[\"LS_3\"] \\\n",
    "    + ((eq_df[\"LS_2\"] - eq_df[\"LS_3\"]) * (ts_df[\"LS_2\"] - ts_df[\"LS_3\"]))\n",
    "\n",
    "cumulative['DS_0'] = 1-cumulative['LS_0']\n",
    "cumulative['DS_1'] = cumulative['LS_0'] - cumulative['LS_1']\n",
    "cumulative['DS_2'] = cumulative['LS_1'] - cumulative['LS_2']\n",
    "cumulative['DS_3'] = cumulative['LS_2'] - cumulative['LS_3']\n",
    "cumulative['DS_4'] = cumulative['LS_3']\n",
    "\n",
    "cumulative['hazard'] = 'Earthquake+Tsunami'\n",
    "path_to_cumulative = os.path.join(path_to_output, 'electric_cumulative_{}yr.csv' .format(ret_prd))\n",
    "cumulative.to_csv(path_to_cumulative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electric MC Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_dmg_result = Dataset.from_file(path_to_cumulative, data_type='incore:epfDamage')\n",
    "path_to_mc_results = os.path.join(path_to_output, 'mc_electric_cumulative_{}yr' .format(ret_prd))\n",
    "\n",
    "mc = MonteCarloFailureProbability(client)\n",
    "mc.set_input_dataset(\"damage\", elec_dmg_result)\n",
    "mc.set_parameter(\"num_cpu\", 8)\n",
    "mc.set_parameter(\"num_samples\", n_sims)\n",
    "mc.set_parameter(\"damage_interval_keys\", [\"DS_0\", \"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"])\n",
    "mc.set_parameter(\"failure_state_keys\", [\"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"])\n",
    "\n",
    "mc.set_parameter(\"result_name\", path_to_mc_results) # name of csv file with results\n",
    "mc.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore Monte-Carlo Electric Damage Results**\n",
    "\n",
    "Each row is an infrastructure component, each column is a sample from the Monte-Carlo analysis. \n",
    "+ 0: No/Insignificant Damage\n",
    "+ 1: Moderate Damage\n",
    "+ 2: Heavy Damage\n",
    "+ 3: Complete Damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_df = os.path.join(path_to_output, 'mc_electric_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "mc_df = read_MC_results(path_to_df, n_sims)\n",
    "mc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df['avg'] = mc_df.mean(axis=1) # getting average of MC results\n",
    "\n",
    "# reading pyIncore dataset to geodataframe\n",
    "elec_dataset_id = \"5d263f08b9219cf93c056c68\"        # defining building dataset (GIS point layer)\n",
    "elec_gdf = read_pyincore_gdf(client, elec_dataset_id, index='guid')\n",
    "\n",
    "# merging the two above\n",
    "elec_mc_df = pd.merge(elec_gdf, mc_df['avg'], left_index=True, right_index=True)\n",
    "\n",
    "# plotting results\n",
    "plot_gdf_map(elec_mc_df, column='avg', vmin=0, vmax=3, s=40, cmap='RdBu_r', edgecolor='0.5', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Transportation Damage\n",
    "Running road and bridge damage for earthquake, tsunami, and cumulative earthquake+tsunami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Earthquake\n",
    "# Road damage\n",
    "# Initializing road damage and fragility service\n",
    "road_dmg = RoadDamage(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# loading in road dataset\n",
    "road_dataset_id = \"60e5e5cd544e944c3ce37d08\"\n",
    "road_dmg.load_remote_input_dataset(\"roads\", road_dataset_id)\n",
    "\n",
    "# seaside road fragility mappng for EQ\n",
    "mapping_id = \"5d545b0bb9219c0689f1f3f4\" # 5 DS\n",
    "# mapping_id = \"6079ec1aef881f48a4ac586c\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "road_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "road_dmg.set_parameter(\"hazard_type\", 'earthquake')\n",
    "road_dmg.set_parameter(\"num_cpu\", 4)\n",
    "road_dmg.set_parameter(\"fragility_key\", 'pgd')\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'road_eq_{}yr' .format(ret_prd))\n",
    "hazard_id = eq_hazard_dict[ret_prd]\n",
    "\n",
    "road_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "road_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "road_dmg.run_analysis()\n",
    "\n",
    "\n",
    "# Bridge damage\n",
    "# initializing bridge damage and fragility service\n",
    "brdg_dmg = BridgeDamage(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# Seaside bridges\n",
    "bridge_dataset_id = \"60e5e576d3c92a78c893ff69\"\n",
    "brdg_dmg.load_remote_input_dataset(\"bridges\", bridge_dataset_id)\n",
    "\n",
    "# Set analysis parameters\n",
    "\n",
    "# seaside road fragility mappng for EQ\n",
    "mapping_id = \"5d55c3a1b9219c0689f1f898\" # 5 DS\n",
    "# mapping_id = \"6079f5435b609c56229bf85c\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "brdg_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "brdg_dmg.set_parameter(\"hazard_type\", 'earthquake')\n",
    "brdg_dmg.set_parameter(\"num_cpu\", 4)\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'bridge_eq_{}yr' .format(ret_prd))\n",
    "hazard_id = eq_hazard_dict[ret_prd]\n",
    "brdg_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "brdg_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "brdg_dmg.run_analysis()\n",
    "print('Earthquake done.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Tsunami\n",
    "# Road damage\n",
    "# Initializing road damage and fragility service\n",
    "road_dmg = RoadDamage(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# loading road dataset\n",
    "road_dataset_id = \"60e5e5cd544e944c3ce37d08\"\n",
    "road_dmg.load_remote_input_dataset(\"roads\", road_dataset_id)\n",
    "\n",
    "# seaside road fragility mappng for tsunami\n",
    "mapping_id = \"5d274fd8b9219c3c553c71ff\" # 5 DS\n",
    "# mapping_id = \"6079c3bc6799d908861f16d4\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "road_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "road_dmg.set_parameter(\"hazard_type\", 'tsunami')\n",
    "road_dmg.set_parameter(\"num_cpu\", 4)\n",
    "road_dmg.set_parameter(\"fragility_key\", \"Non-Retrofit inundationDepth Fragility ID Code\")\n",
    "\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'road_tsu_{}yr' .format(ret_prd))\n",
    "hazard_id = tsu_hazard_dict[ret_prd]\n",
    "\n",
    "road_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "road_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "road_dmg.run_analysis()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bridge damage\n",
    "\n",
    "# Initialzing bridge damage and fragility service\n",
    "brdg_dmg = BridgeDamage(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# Seaside bridges\n",
    "bridge_dataset_id = \"60e5e576d3c92a78c893ff69\"\n",
    "brdg_dmg.load_remote_input_dataset(\"bridges\", bridge_dataset_id)\n",
    "\n",
    "# seaside road fragility mappng for EQ\n",
    "mapping_id = \"5d275000b9219c3c553c7202\"\t# 5 DS\n",
    "# mapping_id = \"6079c41a6799d908861f16fe\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "brdg_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "brdg_dmg.set_parameter(\"hazard_type\", 'tsunami')\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'bridge_tsu_{}yr' .format(ret_prd))\n",
    "hazard_id = tsu_hazard_dict[ret_prd]\n",
    "brdg_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "brdg_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "brdg_dmg.run_analysis()\n",
    "print('Tsunami done.')\n",
    "\n",
    "\n",
    "# --- Cumualtive\n",
    "print('\\nCumulative transportation damage does not exist in pyIncore yet. Doing manually')\n",
    "\n",
    "# Road Damage\n",
    "\n",
    "path_to_eq = os.path.join(path_to_output, 'road_eq_{}yr.csv' .format(ret_prd))\n",
    "path_to_ts = os.path.join(path_to_output, 'road_tsu_{}yr.csv' .format(ret_prd))\n",
    "\n",
    "eq_df = pd.read_csv(path_to_eq, index_col=0)\n",
    "ts_df = pd.read_csv(path_to_ts, index_col=0)\n",
    "\n",
    "cumulative = pd.DataFrame(index=eq_df.index)\n",
    "cumulative[\"LS_0\"] = eq_df[\"LS_0\"] + ts_df[\"LS_0\"] \\\n",
    "    - eq_df[\"LS_0\"] * ts_df[\"LS_0\"]\n",
    "\n",
    "cumulative[\"LS_1\"] = eq_df[\"LS_1\"] + ts_df[\"LS_1\"] - eq_df[\"LS_1\"] * ts_df[\"LS_1\"] \\\n",
    "    + ((eq_df[\"LS_0\"] - eq_df[\"LS_1\"]) * (ts_df[\"LS_0\"] - ts_df[\"LS_1\"]))\n",
    "\n",
    "cumulative[\"LS_2\"] = eq_df[\"LS_2\"] + ts_df[\"LS_2\"] \\\n",
    "    - eq_df[\"LS_2\"] * ts_df[\"LS_2\"] \\\n",
    "    + ((eq_df[\"LS_1\"] - eq_df[\"LS_2\"]) * (ts_df[\"LS_1\"] - ts_df[\"LS_2\"]))\n",
    "\n",
    "cumulative[\"LS_3\"] = eq_df[\"LS_3\"] + ts_df[\"LS_3\"] \\\n",
    "    - eq_df[\"LS_3\"] * ts_df[\"LS_3\"] \\\n",
    "    + ((eq_df[\"LS_2\"] - eq_df[\"LS_3\"]) * (ts_df[\"LS_2\"] - ts_df[\"LS_3\"]))\n",
    "\n",
    "cumulative['DS_0'] = 1-cumulative['LS_0']\n",
    "cumulative['DS_1'] = cumulative['LS_0'] - cumulative['LS_1']\n",
    "cumulative['DS_2'] = cumulative['LS_1'] - cumulative['LS_2']\n",
    "cumulative['DS_3'] = cumulative['LS_2'] - cumulative['LS_3']\n",
    "cumulative['DS_4'] = cumulative['LS_3']\n",
    "\n",
    "cumulative['hazard'] = 'Earthquake+Tsunami'\n",
    "path_to_cumulative = os.path.join(path_to_output, 'road_cumulative_{}yr.csv' .format(ret_prd))\n",
    "cumulative.to_csv(path_to_cumulative)\n",
    "\n",
    "\n",
    "# Bridge Damage\n",
    "\n",
    "path_to_eq = os.path.join(path_to_output, 'bridge_eq_{}yr.csv' .format(ret_prd))\n",
    "path_to_ts = os.path.join(path_to_output, 'bridge_tsu_{}yr.csv' .format(ret_prd))\n",
    "\n",
    "eq_df = pd.read_csv(path_to_eq, index_col=0)\n",
    "ts_df = pd.read_csv(path_to_ts, index_col=0)\n",
    "\n",
    "cumulative = pd.DataFrame(index=eq_df.index)\n",
    "cumulative[\"LS_0\"] = eq_df[\"LS_0\"] + ts_df[\"LS_0\"] \\\n",
    "    - eq_df[\"LS_0\"] * ts_df[\"LS_0\"]\n",
    "\n",
    "cumulative[\"LS_1\"] = eq_df[\"LS_1\"] + ts_df[\"LS_1\"] - eq_df[\"LS_1\"] * ts_df[\"LS_1\"] \\\n",
    "    + ((eq_df[\"LS_0\"] - eq_df[\"LS_1\"]) * (ts_df[\"LS_0\"] - ts_df[\"LS_1\"]))\n",
    "\n",
    "cumulative[\"LS_2\"] = eq_df[\"LS_2\"] + ts_df[\"LS_2\"] \\\n",
    "    - eq_df[\"LS_2\"] * ts_df[\"LS_2\"] \\\n",
    "    + ((eq_df[\"LS_1\"] - eq_df[\"LS_2\"]) * (ts_df[\"LS_1\"] - ts_df[\"LS_2\"]))\n",
    "\n",
    "cumulative[\"LS_3\"] = eq_df[\"LS_3\"] + ts_df[\"LS_3\"] \\\n",
    "    - eq_df[\"LS_3\"] * ts_df[\"LS_3\"] \\\n",
    "    + ((eq_df[\"LS_2\"] - eq_df[\"LS_3\"]) * (ts_df[\"LS_2\"] - ts_df[\"LS_3\"]))\n",
    "\n",
    "cumulative['DS_0'] = 1-cumulative['LS_0']\n",
    "cumulative['DS_1'] = cumulative['LS_0'] - cumulative['LS_1']\n",
    "cumulative['DS_2'] = cumulative['LS_1'] - cumulative['LS_2']\n",
    "cumulative['DS_3'] = cumulative['LS_2'] - cumulative['LS_3']\n",
    "cumulative['DS_4'] = cumulative['LS_3']\n",
    "\n",
    "cumulative['hazard'] = 'Earthquake+Tsunami'\n",
    "path_to_cumulative = os.path.join(path_to_output, 'bridge_cumulative_{}yr.csv' .format(ret_prd))\n",
    "cumulative.to_csv(path_to_cumulative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transportation MC Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Road\n",
    "path_to_cumulative = os.path.join(path_to_output, 'road_cumulative_{}yr.csv' .format(ret_prd))\n",
    "road_dmg_result = Dataset.from_file(path_to_cumulative, data_type='ergo:roadDamageVer3')\n",
    "path_to_mc_results = os.path.join(path_to_output, 'mc_road_cumulative_{}yr' .format(ret_prd))\n",
    "\n",
    "mc = MonteCarloFailureProbability(client)\n",
    "mc.set_input_dataset(\"damage\", road_dmg_result)\n",
    "mc.set_parameter(\"num_cpu\", 8)\n",
    "mc.set_parameter(\"num_samples\", n_sims)\n",
    "mc.set_parameter(\"damage_interval_keys\", [\"DS_0\", \"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"])\n",
    "mc.set_parameter(\"failure_state_keys\", [\"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"])\n",
    "\n",
    "mc.set_parameter(\"result_name\", path_to_mc_results) # name of csv file with results\n",
    "mc.run()\n",
    "\n",
    "path_to_df = os.path.join(path_to_output, 'mc_road_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "df = read_MC_results(path_to_df, n_sims)\n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "# Bridge\n",
    "path_to_cumulative = os.path.join(path_to_output, 'bridge_cumulative_{}yr.csv' .format(ret_prd))\n",
    "brdg_dmg_result = Dataset.from_file(path_to_cumulative, data_type='ergo:bridgeDamageVer3')\n",
    "path_to_mc_results = os.path.join(path_to_output, 'mc_bridge_cumulative_{}yr' .format(ret_prd))\n",
    "\n",
    "mc = MonteCarloFailureProbability(client)\n",
    "mc.set_input_dataset(\"damage\", brdg_dmg_result)\n",
    "mc.set_parameter(\"num_cpu\", 8)\n",
    "mc.set_parameter(\"num_samples\", n_sims)\n",
    "mc.set_parameter(\"damage_interval_keys\", [\"DS_0\", \"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"])\n",
    "mc.set_parameter(\"failure_state_keys\", [\"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"])\n",
    "\n",
    "mc.set_parameter(\"result_name\", path_to_mc_results) # name of csv file with results\n",
    "mc.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore Monte-Carlo Transportation Damage Results**\n",
    "\n",
    "Only showing road damage results. Each row is a road component, each column is a sample from the Monte-Carlo analysis. \n",
    "+ 0: No/Insignificant Damage\n",
    "+ 1: Moderate Damage\n",
    "+ 2: Heavy Damage\n",
    "+ 3: Complete Damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_df = os.path.join(path_to_output, 'mc_road_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "mc_df = read_MC_results(path_to_df, n_sims)\n",
    "mc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df['avg'] = mc_df.mean(axis=1) # getting average of MC results\n",
    "\n",
    "# reading pyIncore dataset to geodataframe\n",
    "road_dataset_id = \"60e5e5cd544e944c3ce37d08\"\n",
    "road_gdf = read_pyincore_gdf(client, road_dataset_id, index='guid')\n",
    "\n",
    "# merging the two above\n",
    "road_mc_df = pd.merge(road_gdf, mc_df['avg'], left_index=True, right_index=True)\n",
    "\n",
    "# plotting results\n",
    "plot_gdf_map(road_mc_df, column='avg', vmin=0, vmax=3, cmap='RdBu_r', linewidth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Water Damage\n",
    "Running water facility and water pipe damage for earthquake, tsunami, and cumulative earthquake+tsunami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Earthquake\n",
    "# Pipe damage\n",
    "# Initializing pipeline damage and fragility service\n",
    "pipeline_dmg = PipelineDamageRepairRate(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# Seaside pipes\n",
    "pipe_dataset_id = \"60e72f9fd3c92a78c89636c7\"\n",
    "pipeline_dmg.load_remote_input_dataset(\"pipeline\", pipe_dataset_id)\n",
    "\n",
    "\n",
    "# seaside pipe fragility mappng for EQ\n",
    "mapping_id = \"5b47c227337d4a38464efea8\"\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "pipeline_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "pipeline_dmg.set_parameter(\"hazard_type\", 'earthquake')\n",
    "pipeline_dmg.set_parameter(\"fragility_key\",'pgv')\n",
    "# pipeline_dmg.set_parameter(\"use_liquefaction\",True)\n",
    "pipeline_dmg.set_parameter(\"num_cpu\", 4)\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'pipe_eq_{}yr' .format(ret_prd))\n",
    "hazard_id = eq_hazard_dict[ret_prd]\n",
    "\n",
    "pipeline_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "pipeline_dmg.set_parameter(\"result_name\",result_name)\n",
    "\n",
    "# Run pipeline damage analysis\n",
    "result = pipeline_dmg.run_analysis()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Facility damage    \n",
    "# Initializing water facility damage and fragility service\n",
    "wterfclty_dmg = WaterFacilityDamage(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# loading input dataset\n",
    "fclty_dataset_id = \"60e5e91960b3f41243faa3b2\"\n",
    "wterfclty_dmg.load_remote_input_dataset(\"water_facilities\", fclty_dataset_id)\n",
    "\n",
    "# wterfclty Fragility Mapping on incore-service\n",
    "mapping_id = \"5d39e010b9219cc18bd0b0b6\" # 5 DS\n",
    "# mapping_id = \"6079f7825b609c56229bf976\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "wterfclty_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "wterfclty_dmg.set_parameter(\"hazard_type\", 'earthquake')\n",
    "wterfclty_dmg.set_parameter(\"fragility_key\",'pga')\n",
    "wterfclty_dmg.set_parameter(\"num_cpu\", 4)\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'wterfclty_eq_{}yr' .format(ret_prd))\n",
    "hazard_id = eq_hazard_dict[ret_prd]\n",
    "\n",
    "wterfclty_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "wterfclty_dmg.set_parameter(\"result_name\",result_name)\n",
    "\n",
    "# Run pipeline damage analysis\n",
    "result = wterfclty_dmg.run_analysis()\n",
    "print('Earthquake done.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Tsunami damage\n",
    "# Pipe damage\n",
    "# Initialzing pipeline damage and fragility service\n",
    "pipeline_dmg = PipelineDamage(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# Seaside pipes\n",
    "pipe_dataset_id = \"60e72f9fd3c92a78c89636c7\"\n",
    "pipeline_dmg.load_remote_input_dataset(\"pipeline\", pipe_dataset_id)\n",
    "\n",
    "# seaside pipe fragility mappng for tsunami\n",
    "mapping_id = \"5d320a87b9219c6d66398b45\" # 5 DS\n",
    "# mapping_id = \"6079b7156799d908861e413b\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "pipeline_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "pipeline_dmg.set_parameter(\"hazard_type\", 'tsunami')\n",
    "pipeline_dmg.set_parameter(\"fragility_key\", \"Non-Retrofit inundationDepth Fragility ID Code\")\n",
    "pipeline_dmg.set_parameter(\"num_cpu\", 4)\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'pipe_tsu_{}yr' .format(ret_prd))\n",
    "hazard_id = tsu_hazard_dict[ret_prd]\n",
    "\n",
    "pipeline_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "pipeline_dmg.set_parameter(\"result_name\",result_name)\n",
    "\n",
    "# Run pipeline damage analysis\n",
    "result = pipeline_dmg.run_analysis()\n",
    "\n",
    "\n",
    "\n",
    "# Facility damage\n",
    "# initialzing water facility damage and fragility service\n",
    "wterfclty_dmg = WaterFacilityDamage(client)\n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# loading input dataset\n",
    "fclty_dataset_id = \"60e5e91960b3f41243faa3b2\"\n",
    "wterfclty_dmg.load_remote_input_dataset(\"water_facilities\", fclty_dataset_id)\n",
    "\n",
    "# wterfclty Fragility Mapping on incore-service\n",
    "mapping_id = \"5d31f737b9219c6d66398521\" # 5 DS\n",
    "# mapping_id = \"6079b8a66799d908861e4bf0\" # 4 DS\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "wterfclty_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "wterfclty_dmg.set_parameter(\"hazard_type\", 'tsunami')\n",
    "wterfclty_dmg.set_parameter(\"fragility_key\",\"Non-Retrofit inundationDepth Fragility ID Code\")\n",
    "wterfclty_dmg.set_parameter(\"num_cpu\", 4)\n",
    "\n",
    "\n",
    "result_name = os.path.join(path_to_output, 'wterfclty_tsu_{}yr' .format(ret_prd))\n",
    "hazard_id = tsu_hazard_dict[ret_prd]\n",
    "\n",
    "wterfclty_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "wterfclty_dmg.set_parameter(\"result_name\",result_name)\n",
    "\n",
    "# Run facility damage analysis\n",
    "result = wterfclty_dmg.run_analysis()\n",
    "print('Tsunami done.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Cumulative\n",
    "print('\\nCumulative water damage does not exist in pyIncore yet.')\n",
    "\n",
    "\n",
    "# Pipeline damage\n",
    "\"\"\" doing MC sampling here for pipes\n",
    "    returns mc results:\n",
    "        0: failed\n",
    "        1: working\n",
    "\"\"\"\n",
    "\n",
    "path_to_eq = os.path.join(path_to_output, 'pipe_eq_{}yr.csv' .format(ret_prd))\n",
    "path_to_ts = os.path.join(path_to_output, 'pipe_tsu_{}yr.csv' .format(ret_prd))\n",
    "\n",
    "eq_df = pd.read_csv(path_to_eq, index_col=0)\n",
    "ts_df = pd.read_csv(path_to_ts, index_col=0)\n",
    "\n",
    "\n",
    "\"\"\" pyincore returns failure probs for EQ.\n",
    "    assuming pipe has failed if in extensive or complete for tsunami. From HAZUS TMTM\n",
    "        Slight: pipeline exposedd due soil erosion; no leakage\n",
    "        Moderate: some displacement in pipeline, requiring minor repairs\n",
    "        Extensive: pipeline damaged (mainly joint failure) resulting in leakage\n",
    "        Complete: multiple pipeline separations at joint and barrel, requiring replacement\n",
    "\"\"\"\n",
    "ts_df['failprob'] = ts_df['DS_3'] + ts_df['DS_4']\n",
    "\n",
    "cumulative_failure_df = pd.DataFrame(index=eq_df.index)\n",
    "for i in range(n_sims):\n",
    "    rv_eq = np.random.uniform(low=0, high=1., size=len(eq_df))\n",
    "    rv_ts = np.random.uniform(low=0, high=1., size=len(ts_df))\n",
    "\n",
    "    eq_mc = rv_eq<eq_df['failprob']\n",
    "    ts_mc = rv_ts<ts_df['failprob']\n",
    "\n",
    "    cumulative_failure_df[i] = np.logical_or(eq_mc.values, ts_mc.values).astype(int)\n",
    "path_to_mc_results = os.path.join(path_to_output, 'mc_wterpipe_cumulative_{}yr.csv' .format(ret_prd))\n",
    "cumulative_failure_df.to_csv(path_to_mc_results)\n",
    "\n",
    "# Facility Damage\n",
    "\n",
    "path_to_eq = os.path.join(path_to_output, 'wterfclty_eq_{}yr.csv' .format(ret_prd))\n",
    "path_to_ts = os.path.join(path_to_output, 'wterfclty_tsu_{}yr.csv' .format(ret_prd))\n",
    "\n",
    "eq_df = pd.read_csv(path_to_eq, index_col=0)\n",
    "ts_df = pd.read_csv(path_to_ts, index_col=0)\n",
    "\n",
    "cumulative = pd.DataFrame(index=eq_df.index)\n",
    "cumulative[\"LS_0\"] = eq_df[\"LS_0\"] + ts_df[\"LS_0\"] \\\n",
    "    - eq_df[\"LS_0\"] * ts_df[\"LS_0\"]\n",
    "\n",
    "cumulative[\"LS_1\"] = eq_df[\"LS_1\"] + ts_df[\"LS_1\"] - eq_df[\"LS_1\"] * ts_df[\"LS_1\"] \\\n",
    "    + ((eq_df[\"LS_0\"] - eq_df[\"LS_1\"]) * (ts_df[\"LS_0\"] - ts_df[\"LS_1\"]))\n",
    "\n",
    "cumulative[\"LS_2\"] = eq_df[\"LS_2\"] + ts_df[\"LS_2\"] \\\n",
    "    - eq_df[\"LS_2\"] * ts_df[\"LS_2\"] \\\n",
    "    + ((eq_df[\"LS_1\"] - eq_df[\"LS_2\"]) * (ts_df[\"LS_1\"] - ts_df[\"LS_2\"]))\n",
    "\n",
    "cumulative[\"LS_3\"] = eq_df[\"LS_3\"] + ts_df[\"LS_3\"] \\\n",
    "    - eq_df[\"LS_3\"] * ts_df[\"LS_3\"] \\\n",
    "    + ((eq_df[\"LS_2\"] - eq_df[\"LS_3\"]) * (ts_df[\"LS_2\"] - ts_df[\"LS_3\"]))\n",
    "\n",
    "cumulative['DS_0'] = 1-cumulative['LS_0']\n",
    "cumulative['DS_1'] = cumulative['LS_0'] - cumulative['LS_1']\n",
    "cumulative['DS_2'] = cumulative['LS_1'] - cumulative['LS_2']\n",
    "cumulative['DS_3'] = cumulative['LS_2'] - cumulative['LS_3']\n",
    "cumulative['DS_4'] = cumulative['LS_3']\n",
    "\n",
    "cumulative['hazard'] = 'Earthquake+Tsunami'\n",
    "path_to_cumulative = os.path.join(path_to_output, 'wterfclty_cumulative_{}yr.csv' .format(ret_prd))\n",
    "cumulative.to_csv(path_to_cumulative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Water MC Sampling\n",
    "<br>\n",
    "For water facilities. MC for pipes was done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water Facility\n",
    "path_to_cumulative = os.path.join(path_to_output, 'wterfclty_cumulative_{}yr.csv' .format(ret_prd))\n",
    "fclty_dmg_result = Dataset.from_file(path_to_cumulative, data_type='ergo:waterFacilityDamageVer6')\n",
    "path_to_mc_results = os.path.join(path_to_output, 'mc_wterfclty_cumulative_{}yr' .format(ret_prd))\n",
    "\n",
    "mc = MonteCarloFailureProbability(client)\n",
    "mc.set_input_dataset(\"damage\", fclty_dmg_result)\n",
    "mc.set_parameter(\"num_cpu\", 8)\n",
    "mc.set_parameter(\"num_samples\", n_sims)\n",
    "mc.set_parameter(\"damage_interval_keys\", [\"DS_0\", \"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"])\n",
    "mc.set_parameter(\"failure_state_keys\", [\"DS_1\", \"DS_2\", \"DS_3\", \"DS_4\"])\n",
    "\n",
    "mc.set_parameter(\"result_name\", path_to_mc_results) # name of csv file with results\n",
    "mc.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore Monte-Carlo Water Damage Results**\n",
    "\n",
    "Only showing pipe failure. Each row is a pipe component, each column is a sample from the Monte-Carlo analysis. \n",
    "+ 0: Failed\n",
    "+ 1: Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_df = os.path.join(path_to_output, 'mc_wterpipe_cumulative_{}yr.csv' .format(ret_prd))\n",
    "mc_df = pd.read_csv(path_to_df)\n",
    "mc_df.set_index('guid', inplace=True)\n",
    "mc_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_df['avg'] = mc_df.mean(axis=1) # getting average of MC results\n",
    "\n",
    "# reading pyIncore dataset to geodataframe\n",
    "pipe_dataset_id = \"60e72f9fd3c92a78c89636c7\"\n",
    "pipe_gdf = read_pyincore_gdf(client, pipe_dataset_id, index='guid')\n",
    "\n",
    "# merging the two above\n",
    "pipe_mc_df = pd.merge(pipe_gdf, mc_df['avg'], left_index=True, right_index=True)\n",
    "\n",
    "# plotting results\n",
    "plot_gdf_map(pipe_mc_df, column='avg', vmin=0, vmax=1, cmap='RdBu', linewidth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "***\n",
    "## 3) Functionality\n",
    "\n",
    "![title](images/Flowchart_3.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3a'></a>\n",
    "### 3a) Functionality Models\n",
    "\n",
    "Here, functionality of each infrastructure system is at the parcel-level and is defined as how each respective infrastructure system performs at every parcel. For example, the electric functionality at a particular parcel is defined as whether the parcel has electricity or not. Similarly, the water functionality at a particular parcel is defined as whether the parcel has running water or not.\n",
    "<br><br>\n",
    "The following code sets up the electric, transportation, and water functionality models. Each model returns a table. The rows correspond to parcels and the columns correspond to a single MC iteration. A binary variable representing functional (1) or not functional (0) is provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating some functions that will be used for each infrastructure system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pyincore_nx(client, ds_id, transportation=False):\n",
    "    \"\"\" reading pyincore dataset and loading into networkx graph \n",
    "    \"\"\"\n",
    "    ds = Dataset.from_data_service(ds_id, data_service)\n",
    "    df = ds.get_dataframe_from_shapefile()\n",
    "    df = df.set_crs(epsg=4326)  # setting dataframe crs\n",
    "    df = df.to_crs(26910)       # converting such that crs units is in meters, not lat/long\n",
    "\n",
    "\n",
    "    temp = df.geometry.length\n",
    "    temp = temp/.3048               # converting meters to feet \n",
    "    df['Length_miles'] = temp/5280  # converting feet to miles\n",
    "    if transportation == True:\n",
    "        df['Speed'] = 30            # assuming 30 mph roads in Seaside\n",
    "        df['travel_time'] = df['Length_miles']/df['Speed'] # assuming free flow travel times\n",
    "        \n",
    "        G_nx = nx.from_pandas_edgelist(df=df, \n",
    "                                        source='fnode_guid', \n",
    "                                        target='tnode_guid', \n",
    "                                        edge_key = 'guid',\n",
    "                                        edge_attr=['guid', 'Length_miles', 'Speed', 'travel_time'], \n",
    "                                        )\n",
    "\n",
    "    else:\n",
    "        G_nx = nx.from_pandas_edgelist(df=df, \n",
    "                                        source='fnode_guid', \n",
    "                                        target='tnode_guid', \n",
    "                                        edge_key = 'guid',\n",
    "                                        edge_attr=['guid', 'Length_miles'], \n",
    "                                        )\n",
    "    G_nx = G_nx.to_undirected()\n",
    "    return G_nx\n",
    "\n",
    "def travel_time_calc(p, sources=None, targets=None):\n",
    "    \"\"\" returns travel time calculation.\n",
    "        Sum of shortest path between all source and target nodes\n",
    "\n",
    "        note that p is a dictionary defined by:\n",
    "            p = dict(nx.shortest_path_length(G_nx, weight='len_km'))\n",
    "    \"\"\"\n",
    "    if sources is None:\n",
    "        sources = p.keys()\n",
    "    if targets is None:\n",
    "        targets = p.keys()\n",
    "\n",
    "    tot = 0\n",
    "    for o_node in sources:\n",
    "        for d_node in targets:\n",
    "            if o_node == d_node:\n",
    "                continue\n",
    "            dist = p[o_node][d_node]\n",
    "            tot += dist\n",
    "\n",
    "    return tot\n",
    "\n",
    "def path_length_iterator(G, sources, targets, weight):\n",
    "    \"\"\" returns iterator \n",
    "        taken from networkx all_pairs_dijkstra_path_length.\n",
    "        -modified such that it loops through sources as opposed to all nodes\n",
    "    \"\"\"\n",
    "    length = nx.shortest_paths.weighted._dijkstra_multisource\n",
    "    weight = nx.shortest_paths.weighted._weight_function(G, weight=weight)\n",
    "    for s in sources:\n",
    "        yield (s, length(G, sources=[s], weight=weight))\n",
    "\n",
    "\n",
    "\n",
    "def find_in_list_of_list(mylist, char):\n",
    "    for sub_list in mylist:\n",
    "        if char in sub_list:\n",
    "            return mylist.index(sub_list)\n",
    "        \n",
    "\n",
    "def restoration_curve(DS, means, stds, save_days, functionality_percentage=None):\n",
    "    \"\"\" uses restoration curves to evaluate the \n",
    "        functionality of infrastructure components \n",
    "        at a particular day after disruption\n",
    "    \"\"\"\n",
    "    n_DS = len(means)    \n",
    "    Res = {}\n",
    "    dists = [st.norm(loc=means[i], scale=stds[i]) for i in range(n_DS)]\n",
    "    for day in save_days:\n",
    "        Res[day] = DS.copy()\n",
    "        replace_dict = {i+1:dists[i].cdf(day) for i in range(n_DS)}\n",
    "        replace_dict[0] = 1  # adding 100% functional at for no damage\n",
    "        Res[day] = Res[day].replace(replace_dict) #.values[:,0]\n",
    "        if functionality_percentage != None:\n",
    "            Res[day] = Res[day]>functionality_percentage\n",
    "    return Res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building Functionality Model\n",
    "\n",
    "Building functionality is defined as whether the sampled damage state results from the Monte-Carlo simulation are below a pre-defined damage state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_func(mc_dmg, func_ds=1):\n",
    "    \"\"\" building functionality calculations.\n",
    "        buildings are assumed functional is the damage state is less than or equal to 'func_ds'\n",
    "    \"\"\"\n",
    "    func = bldg_mc_dmg<=func_ds\n",
    "    func = func.astype(int)\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Electric Functionality Model\n",
    "\n",
    "Electric functionality is a binary value of 0 (no electricity) or 1 (has electricity). Pole failure probabilities are associated with each damage state. If a pole is failed, it is removed from the network. If a building is connected to the substation via the remaining electric lines, it has electricity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def electric_func(G, mc_dmg, bldg2ntwk_df, elec_pole_gdf, save_days=[1]):\n",
    "    np.random.seed(1337)\n",
    "    \n",
    "    # allocating space to save results\n",
    "    bldg_conn_save = {}\n",
    "    H_dict = {}\n",
    "\n",
    "    # getting pole functionality at day 1 from restoration curves\n",
    "    means = [0.3, 1.0, 3.0, 7.0]\n",
    "    stds = [0.2, 0.5, 1.5, 3.0]\n",
    "    functionality_percentage=0.8\n",
    "    mc_func = restoration_curve(mc_dmg, means, stds, save_days, functionality_percentage)\n",
    "    mc_fail = {i: ~mc_func[i] for i in mc_func.keys()}\n",
    "\n",
    "    # getting substation functionality at day 1 from restoration curves\n",
    "    ss_guid = elec_pole_gdf.loc[elec_pole_gdf['utilfcltyc']=='ESSL2']['guid'].item()\n",
    "    ss_pole_guid = '557fe1a3-2ad8-494b-bbc1-d5dfccc4dc3c'\n",
    "\n",
    "    means = [1, 3, 7, 30]\n",
    "    stds = [0.5, 1.5, 3.5, 15]\n",
    "    ss_dmg = mc_dmg.loc[ss_guid]\n",
    "    ss_func = restoration_curve(ss_dmg, means, stds, save_days, functionality_percentage)\n",
    "    ss_fail = {i: ~ss_func[i] for i in ss_func.keys()}\n",
    "    \n",
    "    for day in save_days:  # looping through days and computing parcel functionality\n",
    "        bldg_conn_save[day] = pd.DataFrame(index=bldg2ntwk_df.index)\n",
    "        H_dict[day] = {}\n",
    "        # looping through each iteration\n",
    "        for i in range(mc_dmg.shape[1]):\n",
    "            mc_iter = mc_fail[day][i]\n",
    "\n",
    "            remove_poles = mc_iter.loc[mc_iter==True].index.to_list()\n",
    "\n",
    "            # getting graph for iteration\n",
    "            H = G.copy()\n",
    "            H.remove_nodes_from(remove_poles)\n",
    "            bins = list(nx.connected_components(H))\n",
    "\n",
    "            ss_bin = find_in_list_of_list(bins, ss_pole_guid)\n",
    "            if ss_bin==None:  # substation \n",
    "                conn_ss_tf = np.zeros(len(bldg2ntwk_df)).astype(int)\n",
    "            else:\n",
    "                conn_ss_tf = np.isin(bldg2ntwk_df['node_guid'].to_list(), list(bins[ss_bin])).astype(int)\n",
    "\n",
    "            if ss_fail[day][i]: # if substation failed, then no parcels have electricity; overwriting anything previous\n",
    "                conn_ss_tf = np.zeros(len(bldg2ntwk_df)).astype(int)\n",
    "\n",
    "            bldg_conn_save[day][i] = conn_ss_tf\n",
    "            H_dict[day][i] = H\n",
    "        \n",
    "    return bldg_conn_save, H_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transportation Functionality Model\n",
    "\n",
    "Transportation functionality is measured on a scale of 0-1 and is based on the concept of travel time resilience. The transportation functionality is defined at each node in the transportation network and is the ratio of pre- to post-\n",
    "disaster travel times to all other nodes. That is, the functionality of the transportation network at node $o$ is defined as:\n",
    "\n",
    "$$F(t)_{o, trns} = \\frac{\\sum\\limits_{d\\epsilon S} T_{(o,d),0}} {\\sum\\limits_{d\\epsilon S} T_{(o,d),t}} $$\n",
    "\n",
    "Where:\n",
    "+ $F(t)_{o, trns}$: transportation functionality at node $o$ at time $t$\n",
    "+ $d$: is a destination node\n",
    "+ $S$: is all nodes in the transportation network\n",
    "+ $T_{(o,d),t}$: Travel time between origin node $o$ and destination node $d$ at time $t$\n",
    "\n",
    "<br><br><br>\n",
    "Post-disaster travel times along each road/bridge segment are related to HAZUS functionality via:\n",
    "$$T'(t)_{pst} = \\frac{T'_{pre}} {f(t)} $$\n",
    "\n",
    "Where:\n",
    "+ $T'(t)_{pst}$: is the travel time along a road segment after the event at time $t$\n",
    "+ $T'_{pre}$: is the travel time along a road segment before the event. It is assumed this is the distance/speed\n",
    "+ $f(t)$: is the HAZUS functionality and is related to the damage state of a road/bridge segment via repair curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transportation_func(G, mc_dmg, bldg2ntwk_df, day=0, bridge_guids=None, save_days=[1]):\n",
    "    np.random.seed(1337)\n",
    "    bldg_func_save = {}\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\" HAZUS does not provide joint restoration curves. \n",
    "        Assuming a combination of the two by taking the larger of EQ/TSU\n",
    "        \n",
    "        roads:   Using TSU restoration curves for roads because these are longer than all EQ curves\n",
    "        bridges: The following curves are used for each damage state for bridges:\n",
    "                - Slight: TSU\n",
    "                - Moderate: TSU\n",
    "                - Extensive: EQ\n",
    "                - Complete: EQ\n",
    "    \"\"\"\n",
    "    \n",
    "    # road functionality\n",
    "    means = [1, 3, 20, 30]\n",
    "    stds = [0.5, 1.5, 10, 15]\n",
    "    mc_func = restoration_curve(mc_dmg, means, stds, save_days)\n",
    "    \n",
    "    # bridge functionality\n",
    "    means = [1, 4, 75, 230]\n",
    "    stds = [0.5, 2, 42, 110]\n",
    "    bridge_dmg = mc_dmg.loc[bridge_guids]\n",
    "    bridge_func = restoration_curve(bridge_dmg, means, stds, save_days)\n",
    "    \n",
    "    # combining road and bridge functionality\n",
    "    mc_func = {i: bridge_func[i].combine_first(mc_func[i]) for i in mc_func.keys()}\n",
    "    \n",
    "    \n",
    "    for day in save_days:\n",
    "        bldg_func_save[day] = pd.DataFrame(index=list(bldg2ntwk_df.index))\n",
    "      \n",
    "    \n",
    "        # looping through each iteration\n",
    "        for i in range(mc_dmg.shape[1]):       \n",
    "            # first getting an updated graph with increased travel times\n",
    "            H = G.copy()\n",
    "            edges = H.edges(data=True)\n",
    "            for e in edges:\n",
    "                guid = H[e[0]][e[1]]['guid']\n",
    "                t_o = H[e[0]][e[1]]['travel_time']\t# hours\n",
    "                func = mc_func[day].loc[guid][i]\n",
    "                H[e[0]][e[1]]['travel_time_post'] = t_o/func\n",
    "\n",
    "\n",
    "            # now setting up resilience calcs\n",
    "            sources, targets = list(H), list(H)\n",
    "            p_locl_pre = dict(path_length_iterator(H, sources, targets, weight='travel_time'))\n",
    "            p_locl_pst = dict(path_length_iterator(H, sources, targets, weight='travel_time_post'))\n",
    "\n",
    "            node_df = pd.DataFrame(index=list(H)) # dataframe to store resilience calc from each node\n",
    "            node_df['tt_resilience'] = None\n",
    "\n",
    "            for node in list(H): # loop through each node as the source\n",
    "                locl_temp_pre = travel_time_calc(p_locl_pre, sources=[node], targets=targets)   # pre-disaster travel time calc\n",
    "                locl_temp_pst = travel_time_calc(p_locl_pst, sources=[node], targets=targets)   # post-disaster travel time calc\n",
    "                node_df.at[node, 'tt_resilience'] = locl_temp_pre/locl_temp_pst\n",
    "\n",
    "            # merging results to map from nodes to buildings; saving to output dataframe\n",
    "            temp_df = pd.merge(bldg2ntwk_df['node_guid'], node_df, left_on='node_guid', right_index=True)\n",
    "            bldg_func_save[day] = pd.merge(bldg_func_save[day], temp_df['tt_resilience'], left_index=True, right_index=True)\n",
    "            bldg_func_save[day].rename(columns={'tt_resilience': str(i)}, inplace=True)\n",
    "\n",
    "    return bldg_func_save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Water Functionality Model\n",
    "Water functionality is a binary value, measured on a scale of 0-1. A parcel either has water or it does not. The water network's dependency on the electric network is captured here. For the testbed, there is one water treatment plant, and three water pumping stations. Each parcel is assigned to one of the three water pumping stations. \n",
    "\n",
    "Thus, assuming parcel $j$ is assigned to water pumping station $i$, the following conditions must be met for this parcel to have water:\n",
    "1. The electric substation, water treatment plant, and water pumping station $i$ must be operable\n",
    "2. The water treatment plant must be connected to the electric substation (via the electric network)\n",
    "3. Water pumping station $i$ must be connected to the electric substation (via the electric network)\n",
    "4. The water treatment plant must be connected to water pumping station $i$ (via the water network)\n",
    "5. Water pumping station $i$ must be connected to parcel $j$ (via the water network) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def water_func(G_wter, G_elec_dict, mc_fail_pipe, fclty_mc_dmg, elec_mc_dmg, \n",
    "               bldg2ntwk_df, elec_pole_gdf, fclty_gdf, pipe_gdf, save_days=[1]): \n",
    "    \n",
    "    np.random.seed(1337)\n",
    "    bldg_conn_save = {}\n",
    "    \n",
    "    # pipe repair rates\n",
    "    \"\"\" using the probability of failure, rather than leak/break.\n",
    "        assuming that the repair rate is the average of the leak/break\n",
    "        repair rates from hazus.\n",
    "                break   leak.   avg.\n",
    "        > 20\" - 0.33    0.66    0.50\n",
    "        < 20\" - 0.50    1.00    0.75\n",
    "    \"\"\"\n",
    "    pipe_reprate = [0.5, 0.75]   # Fixed pipes per Day per Worker (>20\", <20\" diameter)\n",
    "    n_workers = 32\n",
    "    pipe_unit_len = 20      # assuming unit length of 20m for pipes\n",
    "    pipe_gdf['n_pipes'] = (pipe_gdf['length_km']/pipe_unit_len)*1000\n",
    "    pipe_gdf.loc[pipe_gdf['diameter']>20, 'reprate'] = pipe_reprate[0]\t# num pipes/day/worker\n",
    "    pipe_gdf.loc[pipe_gdf['diameter']<20, 'reprate'] = pipe_reprate[1]\t# num pipes/day/worker\n",
    "    pipe_gdf['reptime'] = ((pipe_gdf['n_pipes']/pipe_gdf['reprate'])/n_workers)*24/16\t# workdays\n",
    "\n",
    "    \n",
    "    # getting wps functionality at day 1 from restoration curves\n",
    "    means = [0.9, 3.1, 13.5, 35]\n",
    "    stds = [0.3, 2.7, 10, 18]\n",
    "    functionality_percentage=0.8\n",
    "    mc_func_fclty = restoration_curve(fclty_mc_dmg, means, stds, save_days, functionality_percentage)\n",
    "    mc_fail_fclty = {i: ~mc_func_fclty[i] for i in mc_func_fclty.keys()}\n",
    "\n",
    "    # getting wtp functionality at day 1 from restoration curves\n",
    "    means = [0.9, 1.9, 32, 95]\n",
    "    stds = [0.3, 1.2, 31, 65]\n",
    "    functionality_percentage=0.8\n",
    "    wtp_guid = fclty_gdf.loc[fclty_gdf['utilfcltyc']=='PWT2'].index.item()\n",
    "    wtp_mc_dmg = fclty_mc_dmg.loc[wtp_guid]\n",
    "    wtp_func = restoration_curve(wtp_mc_dmg, means, stds, save_days, functionality_percentage)\n",
    "    wtp_func = {i: wtp_func[i].to_frame().transpose() for i in wtp_func.keys()}\n",
    "\n",
    "    wtp_fail = {i: ~wtp_func[i] for i in wtp_func.keys()}\n",
    "    mc_func_fclty = {i: wtp_func[i].combine_first(mc_func_fclty[i]) for i in mc_func_fclty.keys()}\n",
    "    mc_fail_fclty = {i: ~mc_func_fclty[i] for i in mc_func_fclty.keys()}\n",
    "    \n",
    "    # getting wtp functionality at day 1 from restoration curves\n",
    "    ss_guid = elec_pole_gdf.loc[elec_pole_gdf['utilfcltyc']=='ESSL2']['guid'].item()\n",
    "    ss_pole_guid = '557fe1a3-2ad8-494b-bbc1-d5dfccc4dc3c'\n",
    "\n",
    "    means = [1, 3, 7, 30]\n",
    "    stds = [0.5, 1.5, 3.5, 15]\n",
    "    ss_dmg = elec_mc_dmg.loc[ss_guid]\n",
    "    mc_func_ss = restoration_curve(ss_dmg, means, stds, save_days, functionality_percentage)\n",
    "    mc_fail_ss = {i: ~mc_func_ss[i] for i in mc_func_ss.keys()}\n",
    "    \n",
    "    #------------------\n",
    "    \n",
    "    for day in save_days:\n",
    "        # getting wtp and wps guid's (both guid associated with facility and with the nearest node)\n",
    "        wtp_guid = fclty_gdf.loc[fclty_gdf['utilfcltyc']=='PWT2'].index.item()\n",
    "        wps_guids = fclty_gdf.loc[fclty_gdf['utilfcltyc']=='PPP2'].index.to_list()\n",
    "        wtp_wter_node_guid = fclty_gdf.loc[fclty_gdf['utilfcltyc']=='PWT2']['node_guid'].item()\n",
    "        wps_wter_node_guid = fclty_gdf.loc[fclty_gdf['utilfcltyc']=='PPP2']['node_guid'].to_list()\n",
    "\n",
    "        bldg_conn_save[day] = pd.DataFrame(index=bldg2ntwk_df.index)\n",
    "\n",
    "        for i in range(pipe_mc_dmg.shape[1]):\n",
    "            G_elec_iter = G_elec_dict[day][i]  # getting damaged electric network as graph (from electric_func())\n",
    "\n",
    "            # assuming everything is operable until determined otherwise\n",
    "            ss_operable_tf = np.ones(len(bldg2ntwk_df)).astype(int)  # substation operable\n",
    "            ss_conn_wtp_tf = np.ones(len(bldg2ntwk_df)).astype(int)  # substation connected to water treatment plant\n",
    "            ss_conn_wps_tf = np.ones(len(bldg2ntwk_df)).astype(int)  # substation connected to water pumping stations\n",
    "            wtp_conn_wps   = np.ones(len(bldg2ntwk_df)).astype(int)  # water treatment plant connected to water pumping station\n",
    "            wps_conn_parcl = np.ones(len(bldg2ntwk_df)).astype(int)  # water pumping station connected to parcel\n",
    "\n",
    "            # --- 1) is electric substation operable?\n",
    "            if mc_fail_ss[day][i]==True:\n",
    "                ss_operable_tf = np.zeros(len(bldg2ntwk_df)).astype(int)\n",
    "\n",
    "            # --- 2) & 3) is the water treatment plant and pumping stations connected to electric substation (via G_elec)?\n",
    "            elec_bins = list(nx.connected_components(G_elec_iter))\n",
    "            ss_bin = find_in_list_of_list(elec_bins, ss_pole_guid)\n",
    "            if ss_bin==None: # getting binary indicator of whether each water facility has electricity\n",
    "                fclty_conn_ss_tf = np.zeros(len(fclty_gdf)).astype(int)\n",
    "            else:\n",
    "                fclty_conn_ss_tf = np.isin(fclty_gdf['elec_node'].to_list(), list(elec_bins[ss_bin])).astype(int)\n",
    "\n",
    "            fclty_gdf['ss_conn'] = fclty_conn_ss_tf  # saving for iteration\n",
    "\n",
    "            # if water treatment plant doesn't have electricity, or is inoperable\n",
    "            if (fclty_gdf.at[wtp_guid, 'ss_conn']==False) or (mc_func_fclty[day].at[wtp_guid, i]==False):\n",
    "                ss_conn_wtp_tf = np.zeros(len(bldg2ntwk_df)).astype(int)\n",
    "\n",
    "            for ii in range(len(wps_wter_node_guid)):    # now, loop through water pumping stations\n",
    "                wps_guid = wps_guids[ii]\n",
    "                wps_node_guid = wps_wter_node_guid[ii]   # getting one water pumping station guid\n",
    "\n",
    "                # getting parcels and indices assigned to this pumping station\n",
    "                parcel_idx = np.where(bldg2ntwk_df['wp_guid']==wps_guid)[0]\n",
    "\n",
    "                # if water pumping station doesn't have electricity, parcels assigned to pumping station don't have water\n",
    "                if fclty_gdf.at[wps_guid, 'ss_conn']==False:  \n",
    "                    ss_conn_wps_tf[parcel_idx] = np.zeros(len(parcel_idx)).astype(int)\n",
    "\n",
    "                # elif facility has electricity and is functional\n",
    "                elif (fclty_gdf.at[wps_guid, 'ss_conn']==True) & (mc_func_fclty[day].at[wps_guid, i]==True): \n",
    "                    ss_conn_wps_tf[parcel_idx] = np.ones(len(parcel_idx)).astype(int)\n",
    "\n",
    "\n",
    "            # remove failed pipes from water graph\n",
    "            mc_iter = mc_fail_pipe[str(i)]\n",
    "            mc_iter = mc_iter.loc[mc_iter==True]\n",
    "            mc_iter = pd.merge(mc_iter, pipe_gdf['reptime'], left_index=True, right_index=True)\n",
    "            mc_iter = mc_iter.sample(frac=1, random_state=1337+i)\n",
    "            mc_iter['cumulative_reptime'] = np.cumsum(mc_iter['reptime'])\n",
    "            remove_pipes = mc_iter.loc[mc_iter['cumulative_reptime']>=day].index.to_list()\n",
    "\n",
    "            # getting water graph for iteration\n",
    "            H = G_wter.copy()\n",
    "            edges = H.edges(data=True)\n",
    "            ebunch = [(e[0],e[1]) for e in edges if e[2]['guid'] in remove_pipes]\n",
    "            H.remove_edges_from(ebunch)\n",
    "\n",
    "            # --- 4) water treatment plant connected to water pumping station i (via G_wter) \n",
    "            wter_bins = list(nx.connected_components(H))\n",
    "            wtp_bin = find_in_list_of_list(wter_bins, wtp_wter_node_guid) # water treatment plant bin\n",
    "\n",
    "            if wtp_bin == None:\n",
    "                fclty_conn_wtp_tf = np.zeros(len(wps_wter_node_guid)).astype(int)\n",
    "            else:\n",
    "                fclty_conn_wtp_tf = np.isin(wps_wter_node_guid, list(wter_bins[wtp_bin])).astype(int)\n",
    "            wtp_idx = np.where(fclty_gdf['utilfcltyc']=='PWT2')[0][0]\n",
    "\n",
    "            fclty_conn_wtp_tf = np.insert(fclty_conn_wtp_tf, wtp_idx, 1) # inserting 1 b/c wtp is connected to wtp\n",
    "            fclty_gdf['wps_conn'] = fclty_conn_wtp_tf  # saving water facility connection results temporarily\n",
    "\n",
    "\n",
    "            # --- 5) water pumping station i connected to parcel j (via G_wter)\n",
    "            for ii in range(len(wps_wter_node_guid)):\n",
    "                wps_guid = wps_guids[ii]  # getting wps guid and node guid\n",
    "                wps_node_guid = wps_wter_node_guid[ii]\n",
    "\n",
    "                # getting parcels (and indices) assigned to this water pumping station\n",
    "                parcel_nodes = bldg2ntwk_df.loc[bldg2ntwk_df['wp_guid']==wps_guid]['node_guid'].to_list()\n",
    "\n",
    "                parcel_idx = np.where(bldg2ntwk_df['wp_guid']==wps_guid)[0]\n",
    "\n",
    "                wps_bin = find_in_list_of_list(wter_bins, wps_node_guid)  # the water bin that the pumping station is in\n",
    "                if wps_bin==None:  # substation \n",
    "                    wps_conn_parcl[parcel_idx] = np.zeros(len(parcel_nodes)).astype(int)\n",
    "                else:\n",
    "                    wps_conn_parcl[parcel_idx] = np.isin(parcel_nodes, list(wter_bins[wps_bin])).astype(int)\n",
    "\n",
    "            conn_wter_tf = ss_operable_tf & ss_conn_wtp_tf & ss_conn_wps_tf & wtp_conn_wps & wps_conn_parcl\n",
    "            bldg_conn_save[day][i] = conn_wter_tf\n",
    "\n",
    "    \n",
    "    return bldg_conn_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3b'></a>\n",
    "### 3b) Functionality of Physical Infrastructure\n",
    "Using the above functionality modules to determine the functionality of each infrastructure system at every parcel. Note that as \"functionality\" is defined as a binary variable at every parcel for each infrastructure system (e.g., does parcel *j* have electricity, does parcel *j* have water, *etc.*), the following plots show the probability of each parcel being functional. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_df = os.path.join(path_to_output, 'mc_buildings_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "bldg_mc_dmg = read_MC_results(path_to_df, n_sims)\n",
    "\n",
    "bldg_func = building_func(bldg_mc_dmg, func_ds=1)\n",
    "\n",
    "# plotting average functionality\n",
    "bldg_func['bldg_avg'] = bldg_func.mean(axis=1)\n",
    "if 'bldg_avg' in bldg_df:\n",
    "    del bldg_df['bldg_avg']\n",
    "bldg_df = pd.merge(bldg_df, bldg_func['bldg_avg'], left_index=True, right_index=True)\n",
    "\n",
    "plot_gdf_map(bldg_df, column='bldg_avg', vmin=0, vmax=1, cmap='magma', s=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Electric Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "elec_ntwk_id = '60e5e326544e944c3ce37a93'\n",
    "elec_pole_id = \"5d263f08b9219cf93c056c68\"\n",
    "bldg2elec_id = '60e5e40c544e944c3ce37ae3'\n",
    "\n",
    "G_elec = read_pyincore_nx(client, elec_ntwk_id)\n",
    "bldg2elec_df = read_pyincore_df(client, bldg2elec_id, index='bldg_guid')\n",
    "elec_pole_gdf = read_pyincore_gdf(client, elec_pole_id)\n",
    "\n",
    "path_to_df = os.path.join(path_to_output, 'mc_electric_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "elec_mc_dmg = read_MC_results(path_to_df, n_sims)\n",
    "\n",
    "# computing electric functionality\n",
    "elec_func, G_elec_dict = electric_func(\n",
    "                            G=G_elec, \n",
    "                            mc_dmg=elec_mc_dmg, \n",
    "                            bldg2ntwk_df=bldg2elec_df, \n",
    "                            elec_pole_gdf=elec_pole_gdf)\n",
    "elec_func[1].mean(axis=1)\n",
    "\n",
    "# plotting average functionality\n",
    "elec_func[1]['elec_avg'] = elec_func[1].mean(axis=1)\n",
    "if 'elec_avg' in bldg_df:\n",
    "    del bldg_df['elec_avg']\n",
    "bldg_df = pd.merge(bldg_df, elec_func[1]['elec_avg'], left_index=True, right_index=True)\n",
    "\n",
    "plot_gdf_map(bldg_df, column='elec_avg', vmin=0, vmax=1, cmap='magma', s=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transportation Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining trans ID's\n",
    "trns_ntwk_id = '60e5e5cd544e944c3ce37d08'\n",
    "bldg2trns_id = '60e5e75c544e944c3ce380cf'\n",
    "bridge_id = '60e5e576d3c92a78c893ff69'\n",
    "\n",
    "# reading in trans data\n",
    "G_trns = read_pyincore_nx(client, trns_ntwk_id, transportation=True)\n",
    "\n",
    "bldg2trns_df = read_pyincore_df(client, bldg2trns_id, index='bldg_guid')\n",
    "bridge_gdf = read_pyincore_gdf(client, bridge_id)\n",
    "\n",
    "# setting up dict that maps bridge to road segments\n",
    "brdg2road_dict = pd.Series(bridge_gdf.link_guid.values, index=bridge_gdf.guid).to_dict()\n",
    "\n",
    "# reading in MC results\n",
    "path_to_df = os.path.join(path_to_output, 'mc_road_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "road_mc_dmg = read_MC_results(path_to_df, n_sims)\n",
    "\n",
    "path_to_df = os.path.join(path_to_output, 'mc_bridge_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "bridge_mc_dmg = read_MC_results(path_to_df, n_sims)\n",
    "\n",
    "# bridge_guids = list(bridge_mc_dmg.index)\n",
    "bridge_guids = list(brdg2road_dict.values())\n",
    "\n",
    "# replacing bridge guid with the road guid that it's on\n",
    "bridge_mc_dmg = bridge_mc_dmg.reset_index().replace({\"index\": brdg2road_dict}).set_index('guid')\n",
    "\n",
    "# combining bridge and road damage\n",
    "trns_mc_dmg = bridge_mc_dmg.combine_first(road_mc_dmg)\n",
    "\n",
    "# computing transportation functionality\n",
    "trns_func = transportation_func(\n",
    "                    G=G_trns, \n",
    "                    mc_dmg=trns_mc_dmg, \n",
    "                    bldg2ntwk_df=bldg2trns_df, \n",
    "                    day=0, \n",
    "                    bridge_guids=bridge_guids\n",
    "                    )\n",
    "\n",
    "# plotting average functionality\n",
    "trns_func[1]['trns_avg'] = trns_func[1].mean(axis=1)\n",
    "if 'trns_avg' in bldg_df:\n",
    "    del bldg_df['trns_avg']\n",
    "bldg_df = pd.merge(bldg_df, trns_func[1]['trns_avg'], left_index=True, right_index=True)\n",
    "\n",
    "plot_gdf_map(bldg_df, column='trns_avg', vmin=0, vmax=1, cmap='magma', s=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Water Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wter_ntwk_id = '60e72f9fd3c92a78c89636c7'\n",
    "wter_fclty_id = '60e5e91960b3f41243faa3b2'\n",
    "bldg2wter_id = '60e5e7dbd3c92a78c8940505'\n",
    "\n",
    "elec_ntwk_id = '60e5e326544e944c3ce37a93'\n",
    "elec_pole_id = \"5d263f08b9219cf93c056c68\"\n",
    "\n",
    "\n",
    "G_wter = read_pyincore_nx(client, wter_ntwk_id)\n",
    "G_elec = read_pyincore_nx(client, elec_ntwk_id)\n",
    "wter_fclty_gdf = read_pyincore_gdf(client, wter_fclty_id, index='guid')\n",
    "wter_pipe_gdf = read_pyincore_gdf(client, wter_ntwk_id, index='guid')\n",
    "bldg2wter_df = read_pyincore_df(client, bldg2wter_id, index='bldg_guid')\n",
    "elec_pole_gdf = read_pyincore_gdf(client, elec_pole_id)\n",
    "\n",
    "path_to_mc_pipe = os.path.join(path_to_output, 'mc_wterpipe_cumulative_{}yr.csv' .format(ret_prd))\n",
    "path_to_mc_fclty= os.path.join(path_to_output, 'mc_wterfclty_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "path_to_mc_elec = os.path.join(path_to_output, 'mc_electric_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "\n",
    "pipe_mc_dmg = pd.read_csv(path_to_mc_pipe, index_col=0)\n",
    "fclty_mc_dmg = read_MC_results(path_to_mc_fclty, n_sims)\n",
    "elec_mc_dmg = read_MC_results(path_to_mc_elec, n_sims)\n",
    "\n",
    "wter_func = water_func(\n",
    "                G_wter=G_wter, \n",
    "                G_elec_dict=G_elec_dict, \n",
    "                mc_fail_pipe=pipe_mc_dmg, \n",
    "                fclty_mc_dmg=fclty_mc_dmg, \n",
    "                elec_mc_dmg=elec_mc_dmg,\n",
    "                bldg2ntwk_df=bldg2wter_df, \n",
    "                elec_pole_gdf=elec_pole_gdf, \n",
    "                fclty_gdf=wter_fclty_gdf,\n",
    "                pipe_gdf=wter_pipe_gdf,\n",
    "                )\n",
    "\n",
    "\n",
    "# plotting average functionality\n",
    "wter_func[1]['wter_avg'] = wter_func[1].mean(axis=1)\n",
    "if 'wter_avg' in bldg_df:\n",
    "    del bldg_df['wter_avg']\n",
    "bldg_df = pd.merge(bldg_df, wter_func[1]['wter_avg'], left_index=True, right_index=True)\n",
    "\n",
    "plot_gdf_map(bldg_df, column='wter_avg', vmin=0, vmax=1, cmap='magma', s=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3c'></a>\n",
    "### 3c) CGE Model\n",
    "Using the Seaside CGE model to determine indirect economic losses within Seaside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting building failure results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bldg_dataset_id = \"613ba5ef5d3b1d6461e8c415\"        # defining building dataset (GIS point layer)\n",
    "building_inv = Dataset.from_data_service(bldg_dataset_id, data_service)\n",
    "\n",
    "path_to_building_fail = os.path.join(path_to_output, \n",
    "                                     'mc_buildings_cumulative_{}yr_failure_probability.csv' .format(ret_prd))\n",
    "df_bldg_fail = pd.read_csv(path_to_building_fail)\n",
    "df_bldg_fail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capital Shock Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_failure_probability = mc_bldg.get_output_dataset('failure_probability')  # get buildings failure probabilities\n",
    "\n",
    "# shock is defined as of percentage reductions in the capital stock of the associated sectors.  \n",
    "capital_shocks = CapitalShocks(client)\n",
    "\n",
    "# Specify the result name\n",
    "\n",
    "result_name = os.path.join(path_to_output, \"Seaside capital losses\")\n",
    "\n",
    "# Set analysis parameters\n",
    "capital_shocks.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "\n",
    "# Load input datasets\n",
    "# Seaside building to sector mapping table\n",
    "building_to_sectors_id = \"5f5fd8e2980a301080a03996\"\n",
    "\n",
    "\n",
    "# Load remote datasets\n",
    "capital_shocks.load_remote_input_dataset(\"buildings_to_sectors\", building_to_sectors_id)\n",
    "\n",
    "# Set datasets\n",
    "# Seaside building inventory\n",
    "capital_shocks.set_input_dataset(\"buildings\", building_inv)  # loading the shapefile directly! \n",
    "\n",
    "# Seaside building failure probability\n",
    "capital_shocks.set_input_dataset(\"failure_probability\", building_failure_probability)\n",
    "capital_shocks.run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showings results from capital shock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_shocks_result = capital_shocks.get_output_dataset(\"sector_shocks\")\n",
    "df = sector_shocks_result.get_dataframe_from_csv()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running CGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "SAM \n",
    "Social accounting matrix (SAM) contains data for firms, households and government which are organized \n",
    "in a way to represent the interactions of all three entities in a typical economy\n",
    "each cell shows payment from its column account to its row account\"\"\"\n",
    "\n",
    "SAM = \"5f17393633b2700c11feab38\"\n",
    "\n",
    "\"\"\"CAPITAL COMP\n",
    "BB is a matrix which describes how investment in physical infrastructure is transformed into functioning capital such as commercial and residential buildings. \n",
    "These data are collected from the Bureau of Economic Analysis (BEA).\"\"\"\n",
    "BB = \"603d23d634f29a7fa41c4c4e\"\n",
    "\n",
    "# MISC TABLES\n",
    "EMPLOY = \"5f173c97c98cf43417c8955d\" # Table name containing data for commercial sector employment.\n",
    "JOBCR = \"5f173d262fab4d660a8e9f9c\" # This is a matrix describing the supply of workers coming from each household group in the economy.\n",
    "HHTABLE = \"5f173d6bc98cf43417c89561\" # HH Table?\n",
    "SIMS = \"5f174211c98cf43417c89565\" # Random numbers for the change of capital stocks in the CGE model.\n",
    "\n",
    "\n",
    "# Create Seaside CGE Model\n",
    "seaside_cge = SeasideCGEModel(client)\n",
    "# Set analysis input datasets\n",
    "seaside_cge.load_remote_input_dataset(\"SAM\", SAM)\n",
    "seaside_cge.load_remote_input_dataset(\"BB\", BB)\n",
    "seaside_cge.load_remote_input_dataset(\"EMPLOY\", EMPLOY)\n",
    "seaside_cge.load_remote_input_dataset(\"JOBCR\", JOBCR)\n",
    "seaside_cge.load_remote_input_dataset(\"HHTABLE\", HHTABLE)\n",
    "seaside_cge.load_remote_input_dataset(\"SIMS\", SIMS)\n",
    "\n",
    "# Set analysis input dataset from previous Capital stock shock analysis\n",
    "seaside_cge.set_input_dataset(\"sector_shocks\", sector_shocks_result)\n",
    "\n",
    "# Run Seaside CGE model analysis\n",
    "seaside_cge.run_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataset containing Seaside cge simulations (format: CSV).\n",
    "# dsc+dsr = domestic supply\n",
    "domestic_supply = seaside_cge.get_output_dataset('Seaside_Sims')\n",
    "df_fp = domestic_supply.get_dataframe_from_csv()\n",
    "df_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3d'></a>\n",
    "### 3d) Social Science Modules\n",
    "\n",
    "Using the population dislocation module in pyIncore to determine whether households are dislocated. Chains results from cumulative building damage and housing unit allocation with the population dislocation method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dis = PopulationDislocation(client)\n",
    "seed = 1111\n",
    "cnt = 0\n",
    "\n",
    "#Reading in local building damage files\n",
    "path_to_dmg = os.path.join(path_to_output, 'buildings_cumulative_{}yr.csv' .format(ret_prd))\n",
    "dmg_data = Dataset.from_file(path_to_dmg, data_type='ergo:buildingDamageVer5')\n",
    "pop_dis.set_input_dataset(\"building_dmg\", dmg_data)\n",
    "\n",
    "# loading remote population dislocation data \n",
    "path_to_hua = os.path.join(path_to_output, 'hua_0_{}.csv' .format(seed))\n",
    "path_to_hua = os.path.join(path_to_output, 'hua_df.csv')\n",
    "hua = Dataset.from_file(path_to_hua,'incore:housingUnitAllocation')\n",
    "\n",
    "pop_dis.set_input_dataset(\"housing_unit_allocation\", hua)\n",
    "pop_dis.load_remote_input_dataset(\"block_group_data\", \"5d542bd8b9219c0689b90408\")\n",
    "pop_dis.load_remote_input_dataset(\"value_poss_param\", \"60354810e379f22e16560dbd\") \n",
    "\n",
    "# setting population dislocation run information\n",
    "result_name = os.path.join(path_to_output, \"PopDis_results\")\n",
    "pop_dis.set_parameter(\"result_name\", result_name)\n",
    "pop_dis.set_parameter(\"seed\", seed)\n",
    "\n",
    "# running population dislocation\n",
    "pop_dis.run_analysis()\n",
    "\n",
    "# getting population dislocation results\n",
    "PopDisResult = pop_dis.get_output_dataset(\"result\")\n",
    "PopDis_df = PopDisResult.get_dataframe_from_csv()\n",
    "PopDis_df.set_index(\"guid\", inplace=True)\n",
    "\n",
    "# merging with bldg_df\n",
    "if 'dislocated' in bldg_df:\n",
    "    del bldg_df['dislocated']\n",
    "bldg_df = pd.merge(bldg_df, PopDis_df['dislocated'], left_index=True, right_index=True)\n",
    "\n",
    "PopDis_df['dislocated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3e'></a>\n",
    "### 3e) Direct and Indirect Economic & Social Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting results from population dislocation analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gdf_map(bldg_df, column='dislocated', category=True, cmap='coolwarm', s=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Showing table of results from CGE analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataset of changes in employment and supply. (format: CSV).\n",
    "# Employment (unit = person)\n",
    "gross_income = seaside_cge.get_output_dataset('Seaside_output')\n",
    "df_fpp = gross_income.get_dataframe_from_csv()\n",
    "df_fpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "***\n",
    "## 4) Recovery\n",
    "\n",
    "<img src=\"images/Flowchart_4.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "### 4) Recovery\n",
    "Setting up a restoration function. In this notebook, HAZUS restoration curves are used. The restoration curves describe the functionality of infrastructure components as a funtion of time.\n",
    "\n",
    "This section of the code loops through the previous section; however, the functionality is modified based on the time in days after the CSZ occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "stop = 1000\n",
    "step = 10\n",
    "save_days = np.arange(start, stop+step, step)\n",
    "if save_days[0] == 0:\n",
    "    save_days[0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Electric Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_ntwk_id = '60e5e326544e944c3ce37a93'\n",
    "elec_pole_id = \"5d263f08b9219cf93c056c68\"\n",
    "bldg2elec_id = '60e5e40c544e944c3ce37ae3'\n",
    "\n",
    "G_elec = read_pyincore_nx(client, elec_ntwk_id)\n",
    "bldg2elec_df = read_pyincore_df(client, bldg2elec_id, index='bldg_guid')\n",
    "elec_pole_gdf = read_pyincore_gdf(client, elec_pole_id)\n",
    "\n",
    "path_to_df = os.path.join(path_to_output, 'mc_electric_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "elec_mc_dmg = read_MC_results(path_to_df, n_sims)\n",
    "\n",
    "# computing electric functionality\n",
    "elec_func, G_elec_dict = electric_func(\n",
    "                            G=G_elec, \n",
    "                            mc_dmg=elec_mc_dmg, \n",
    "                            bldg2ntwk_df=bldg2elec_df, \n",
    "                            elec_pole_gdf=elec_pole_gdf,\n",
    "                            save_days=save_days)\n",
    "\n",
    "elec_mean = []\n",
    "elec_std = []\n",
    "for day in save_days:\n",
    "    elec_func_day = elec_func[day]\n",
    "    elec_mean.append(elec_func_day.sum(axis=0).mean())\n",
    "    elec_std.append(elec_func_day.sum(axis=0).std())\n",
    "    \n",
    "elec_mean = np.array(elec_mean)\n",
    "elec_std = np.array(elec_std)\n",
    "elec_upper = np.clip(elec_mean + elec_std, a_min=0, a_max=len(elec_func[day]))\n",
    "elec_lower = np.clip(elec_mean - elec_std, a_min=0, a_max=len(elec_func[day]))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(save_days, elec_mean, 'k')\n",
    "ax.fill_between(save_days, elec_lower, elec_upper, alpha=0.6)\n",
    "ax.set_xlabel('Time (days)')\n",
    "ax.set_ylabel('Number of Parcels with Electricity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transportation Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining trans ID's\n",
    "trns_ntwk_id = '60e5e5cd544e944c3ce37d08'\n",
    "bldg2trns_id = '60e5e75c544e944c3ce380cf'\n",
    "bridge_id = '60e5e576d3c92a78c893ff69'\n",
    "\n",
    "# reading in trans data\n",
    "G_trns = read_pyincore_nx(client, trns_ntwk_id, transportation=True)\n",
    "\n",
    "bldg2trns_df = read_pyincore_df(client, bldg2trns_id, index='bldg_guid')\n",
    "bridge_gdf = read_pyincore_gdf(client, bridge_id)\n",
    "\n",
    "# setting up dict that maps bridge to road segments\n",
    "brdg2road_dict = pd.Series(bridge_gdf.link_guid.values, index=bridge_gdf.guid).to_dict()\n",
    "\n",
    "# reading in MC results\n",
    "path_to_df = os.path.join(path_to_output, 'mc_road_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "road_mc_dmg = read_MC_results(path_to_df, n_sims)\n",
    "\n",
    "path_to_df = os.path.join(path_to_output, 'mc_bridge_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "bridge_mc_dmg = read_MC_results(path_to_df, n_sims)\n",
    "\n",
    "# bridge_guids = list(bridge_mc_dmg.index)\n",
    "bridge_guids = list(brdg2road_dict.values())\n",
    "\n",
    "# replacing bridge guid with the road guid that it's on\n",
    "bridge_mc_dmg = bridge_mc_dmg.reset_index().replace({\"index\": brdg2road_dict}).set_index('guid')\n",
    "\n",
    "# combining bridge and road damage\n",
    "trns_mc_dmg = bridge_mc_dmg.combine_first(road_mc_dmg)\n",
    "\n",
    "# computing transportation functionality\n",
    "trns_func = transportation_func(\n",
    "                    G=G_trns, \n",
    "                    mc_dmg=trns_mc_dmg, \n",
    "                    bldg2ntwk_df=bldg2trns_df, \n",
    "                    day=0, \n",
    "                    bridge_guids=bridge_guids,\n",
    "                    save_days=save_days\n",
    "                    )\n",
    "\n",
    "\n",
    "trns_func_level = 0.75\n",
    "trns_mean = []\n",
    "trns_std = []\n",
    "for day in save_days:\n",
    "    trns_func_day = trns_func[day]>trns_func_level\n",
    "    trns_mean.append(trns_func_day.sum(axis=0).mean())\n",
    "    trns_std.append(trns_func_day.sum(axis=0).std())\n",
    "    \n",
    "trns_mean = np.array(trns_mean)\n",
    "trns_std = np.array(trns_std)\n",
    "trns_upper = np.clip(trns_mean + trns_std, a_min=0, a_max=len(trns_func[day]))\n",
    "trns_lower = np.clip(trns_mean - trns_std, a_min=0, a_max=len(trns_func[day]))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(save_days, trns_mean, 'k')\n",
    "ax.fill_between(save_days, trns_lower, trns_upper, alpha=0.6)\n",
    "# ax.set_xscale('log')\n",
    "\n",
    "ax.set_xlabel('Time (days)')\n",
    "ax.set_ylabel('Number of Parcels with Transportation Func. Above {}' .format(trns_func_level))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Water Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wter_ntwk_id = '60e72f9fd3c92a78c89636c7'\n",
    "wter_fclty_id = '60e5e91960b3f41243faa3b2'\n",
    "bldg2wter_id = '60e5e7dbd3c92a78c8940505'\n",
    "\n",
    "elec_ntwk_id = '60e5e326544e944c3ce37a93'\n",
    "elec_pole_id = \"5d263f08b9219cf93c056c68\"\n",
    "\n",
    "\n",
    "G_wter = read_pyincore_nx(client, wter_ntwk_id)\n",
    "G_elec = read_pyincore_nx(client, elec_ntwk_id)\n",
    "wter_fclty_gdf = read_pyincore_gdf(client, wter_fclty_id, index='guid')\n",
    "wter_pipe_gdf = read_pyincore_gdf(client, wter_ntwk_id, index='guid')\n",
    "bldg2wter_df = read_pyincore_df(client, bldg2wter_id, index='bldg_guid')\n",
    "elec_pole_gdf = read_pyincore_gdf(client, elec_pole_id)\n",
    "\n",
    "\n",
    "path_to_mc_pipe = os.path.join(path_to_output, 'mc_wterpipe_cumulative_{}yr.csv' .format(ret_prd))\n",
    "path_to_mc_fclty= os.path.join(path_to_output, 'mc_wterfclty_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "path_to_mc_elec = os.path.join(path_to_output, 'mc_electric_cumulative_{}yr_sample_damage_states.csv' .format(ret_prd))\n",
    "\n",
    "pipe_mc_dmg = pd.read_csv(path_to_mc_pipe, index_col=0)\n",
    "fclty_mc_dmg = read_MC_results(path_to_mc_fclty, n_sims)\n",
    "elec_mc_dmg = read_MC_results(path_to_mc_elec, n_sims)\n",
    "\n",
    "wter_func = water_func(\n",
    "                G_wter=G_wter, \n",
    "                G_elec_dict=G_elec_dict, \n",
    "                mc_fail_pipe=pipe_mc_dmg, \n",
    "                fclty_mc_dmg=fclty_mc_dmg, \n",
    "                elec_mc_dmg=elec_mc_dmg,\n",
    "                bldg2ntwk_df=bldg2wter_df, \n",
    "                elec_pole_gdf=elec_pole_gdf, \n",
    "                fclty_gdf=wter_fclty_gdf,\n",
    "                pipe_gdf=wter_pipe_gdf,\n",
    "                save_days=save_days)\n",
    "\n",
    "\n",
    "\n",
    "wter_mean = []\n",
    "wter_std = []\n",
    "for day in save_days:\n",
    "    wter_func_day = wter_func[day]\n",
    "    wter_mean.append(wter_func_day.sum(axis=0).mean())\n",
    "    wter_std.append(wter_func_day.sum(axis=0).std())\n",
    "    \n",
    "wter_mean = np.array(wter_mean)\n",
    "wter_std = np.array(wter_std)\n",
    "wter_upper = np.clip(wter_mean + wter_std, a_min=0, a_max=len(wter_func[day]))\n",
    "wter_lower = np.clip(wter_mean - wter_std, a_min=0, a_max=len(wter_func[day]))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(save_days, wter_mean, 'k')\n",
    "ax.fill_between(save_days, wter_lower, wter_upper, alpha=0.6)\n",
    "# ax.set_xscale('log')\n",
    "\n",
    "ax.set_xlabel('Time (days)')\n",
    "ax.set_ylabel('Number of Parcels with Water')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "***\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Guidotti, R., Gardoni, P., and Rosenheim, N. (2019). Integration of Physical Infrastructure and Social Systems in Communitiesâ€™ Reliability and Resilience Analysis. *Reliability Engineering & System Safety*, 185, 476-492. https://doi.org/10.1016/j.ress.2019.01.008\n",
    "\n",
    "\n",
    "* Kameshwar, S., Cox, D., Barbosa, A., Farokhnia, K., Park, H., Alam, M., and van de Lindt, J. (2019). Probabilistic Decision-Support Framework for Community Resilience: Incorporating Multi-Hazards, Infrastructure Interdependencies, and Resilience Goals in a Bayesian Network. *Reliability Engineering and System Safety*, 191. https://doi.org/10.1016/j.ress.2019.106568\n",
    "\n",
    "\n",
    "* Kameshwar, S., Park, H., Cox, D., and Barbosa, A. (2021). Effect of Disaster Debris, Floodwater Pooling Duration, and Bridge Damage on Immediate Post-Tsunami Connectivity. *International Journal of Disaster Risk Reduction*, 56. https://doi.org/10.1016/j.ijdrr.2021.102119\n",
    "\n",
    "\n",
    "* Park, H., and Cox, D. (2016). Probabilistic Assessment of Near-Field Tsunami Hazards: Inundation Depth, Velocity, Momentum Flux, Arrival Time, and Duration Applied to Seaside, Oregon. *Coastal Engineering*, 117, 79-96. https://doi.org/10.1016/j.coastaleng.2016.07.011\n",
    "\n",
    "\n",
    "* Park, H., Cox, D., and Barbosa, A. (2017a). Comparison of inundation depth and momentum flux based fragilities for probabilistic tsunami damage assessment and uncertainty analysis. *Coastal Engineering*, 122, 10-26. https://doi.org//10.1016/j.coastaleng.2017.01.008\n",
    "\n",
    "\n",
    "* Park, H., Cox, D., Alam, M., and Barbosa, A. (2017b). Probabilistic Seismic and Tsunami Hazard Analysis Conditioned on a Megathrust Rupture of the Cascadia Subduction Zone. *Frontiers in Built Environment*, 3, 32. https://doi.org/10.3389/fbuil.2017.00032\n",
    "\n",
    "\n",
    "* Park, H., Alam, M., Cox, D., Barbosa, A., and van de Lindt, J. (2019). Probabilistic Seismic and Tsunami Damage Analysis (PSTDA) of the Cascadia Subduction Zone Applied to Seaside, Oregon. *International Journal of Disaster Risk Reduction*, 35. https://doi.org/10.1016/j.ijdrr.2019.101076\n",
    "\n",
    "\n",
    "* Rosenheim, N., Guidotti, R., Gardoni, P., and Peacock, W. (2019). Integration of Detailed Household and Housing Unit Characteristic Data with Critical Infrastructure for Post-Hazard Resilience Modeling. *Sustainable and Resilient Infrastructure*, 1-17. https://doi.org/10.1080/23789689.2019.1681821\n",
    "\n",
    "\n",
    "* Sanderson, D., Kameshwar, S., Rosenheim, N., and Cox, D. (2021). Deaggregation of Multi-Hazard Damages, Losses, Risks, and Connectivity: An Application to the Joint Seismic-Tsunami Hazard at Seaside, Oregon. *Natural Hazards*. https://doi.org/10.1007/s11069-021-04900-9\n",
    "\n",
    "\n",
    "* Sanderson, D., Cox, D., and Naraharisetty, G. (2021). A Spatially Explicit Decision Support Framework for Parcel- and Community-Level Resilience Assessment using Bayesian Networks. *Sustainable and Resilient Infrastructure*. https://doi.org/10.1080/23789689.2021.1966164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

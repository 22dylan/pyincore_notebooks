{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Seaside Building Optimization </center></h1>\n",
    "This code performs a multi-hazard damage analysis and optimization of building mitigation options for Seaside, Oregon.\n",
    "\n",
    "\n",
    "This notebook consists of the following sections:\n",
    "+ [Importing modules and defining recurrence interval](#input)\n",
    "+ [Multi-Hazard damage analysis](#dmg_analysis)\n",
    "  + [Direct economic loss computations](#econ_loss)\n",
    "  + [Repair time estimates](#repair)\n",
    "  + [Population dislocation estimates](#disloc)\n",
    "+ [Optimization](#optimization)\n",
    "  + [Data aggregation](#agg-opt)\n",
    "  + [Optimal solution search](#optimal_search)\n",
    "+ [Plotting Results](#plotting)\n",
    "\n",
    "\n",
    "The mitigation options that are considered at each parcel are:\n",
    "\n",
    "| Mitigation option | Description |\n",
    "| --- | --- |\n",
    "| 0 | Do nothing / Status Quo | \n",
    "| 1 | Retrofit structure to high code |\n",
    "| 2 | Relocate structure outside the inundation zone | \n",
    "| 3 | Decrease repair time | \n",
    "\n",
    "*Notebook developed by [Tarun Adluri](https://www.linkedin.com/in/tarunadluri) and [Dylan Sanderson](https://github.com/22dylan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='input'></a>\n",
    "***\n",
    "### Importing modules and defining recurrence interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter username: sanderdy\n",
      "Enter password: ········\n",
      "Connection successful to IN-CORE services. pyIncore version detected: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"]=20,20\n",
    "\n",
    "from pyincore import IncoreClient, Dataset, FragilityService, MappingSet, DataService\n",
    "from pyincore.analyses.buildingdamage import BuildingDamage\n",
    "from pyincore.analyses.cumulativebuildingdamage import CumulativeBuildingDamage\n",
    "from pyincore.analyses.populationdislocation import PopulationDislocation, PopulationDislocationUtil\n",
    "from pyincore.analyses.housingunitallocation import HousingUnitAllocation\n",
    "client = IncoreClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying recurrence interval to consider\n",
    "\n",
    "| Recurrence Intervals Options |\n",
    "| --- | \n",
    "| 100 |  \n",
    "| 250 | \n",
    "| 500 |  \n",
    "| 1,000 |\n",
    "| 2,500 |\n",
    "| 5,000 |\n",
    "| 10,000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select hazard return period in years (e.g. 500): 500\n"
     ]
    }
   ],
   "source": [
    "event = int(input(\"Select hazard return period in years (e.g. 500): \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specifiying pyIncore hazard information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_hazard_dict = {100: \"5dfa4058b9219c934b64d495\", \n",
    "                          250: \"5dfa41aab9219c934b64d4b2\",\n",
    "                          500: \"5dfa4300b9219c934b64d4d0\",\n",
    "                          1000: \"5dfa3e36b9219c934b64c231\",\n",
    "                          2500: \"5dfa4417b9219c934b64d4d3\", \n",
    "                          5000: \"5dfbca0cb9219c101fd8a58d\",\n",
    "                         10000: \"5dfa51bfb9219c934b68e6c2\"}\n",
    "\n",
    "tsunami_hazard_dict = {100: \"5bc9e25ef7b08533c7e610dc\", \n",
    "                      250: \"5df910abb9219cd00cf5f0a5\",\n",
    "                      500: \"5df90e07b9219cd00ce971e7\",\n",
    "                      1000: \"5df90137b9219cd00cb774ec\",\n",
    "                      2500: \"5df90761b9219cd00ccff258\",\n",
    "                      5000: \"5df90871b9219cd00ccff273\",\n",
    "                      10000: \"5d27b986b9219c3c55ad37d0\"}\n",
    "\n",
    "# creating path to damage output\n",
    "path_to_dmg = os.path.join(os.getcwd(), 'damage_results')\n",
    "if not os.path.exists(path_to_dmg):\n",
    "    os.makedirs(path_to_dmg)\n",
    "\n",
    "# creating path to dislocation output\n",
    "path_to_dislocation = os.path.join(os.getcwd(), 'dislocation_results')   \n",
    "if not os.path.exists(path_to_dislocation):\n",
    "    os.makedirs(path_to_dislocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dmg_analysis'></a>\n",
    "***\n",
    "<h2><center> Multi-hazard damage analysis </center><h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Earthquake building damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists locally. Reading from local cached zip.\n",
      "Unzipped folder found in the local cache. Reading from it...\n"
     ]
    }
   ],
   "source": [
    "# initializing building damage and fragility service\n",
    "bldg_dmg = BuildingDamage(client)   \n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# defining building dataset (GIS point layer)\n",
    "bldg_dataset_id = \"5df40388b9219c06cf8b0c80\"\n",
    "bldg_dmg.load_remote_input_dataset(\"buildings\", bldg_dataset_id)\n",
    "\n",
    "retrofit_mapping_ids = [\"5d2789dbb9219c3c553c7977\", \n",
    "                        \"5e99d145f2935b00011900a4\"]\n",
    "\n",
    "for retrofit_i in range(len(retrofit_mapping_ids)):\n",
    "    # specifiying mapping id from fragilites to building types\n",
    "    mapping_id = retrofit_mapping_ids[retrofit_i]\n",
    "    mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "    bldg_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "    bldg_dmg.set_parameter(\"hazard_type\", \"earthquake\")\n",
    "    bldg_dmg.set_parameter(\"num_cpu\", 4)\n",
    "\n",
    "    result_name = os.path.join(path_to_dmg, 'buildings_eq_{}yr_opt{}' .format(event, retrofit_i))\n",
    "    hazard_id = earthquake_hazard_dict[event]\n",
    "    bldg_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "    bldg_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "    bldg_dmg.run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tsunami building damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists locally. Reading from local cached zip.\n",
      "Unzipped folder found in the local cache. Reading from it...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing building damage and fragility service\n",
    "bldg_dmg = BuildingDamage(client)   \n",
    "fragility_service = FragilityService(client)\n",
    "\n",
    "# defining building dataset (GIS point layer)\n",
    "bldg_dataset_id = \"5df40388b9219c06cf8b0c80\"\n",
    "bldg_dmg.load_remote_input_dataset(\"buildings\", bldg_dataset_id)\n",
    "\n",
    "# --- running through tsunami building damage with status quo options\n",
    "# specifiying mapping id from fragilites to building types\n",
    "\n",
    "mapping_id = \"5d279bb9b9219c3c553c7fba\"\n",
    "mapping_set = MappingSet(fragility_service.get_mapping(mapping_id))\n",
    "bldg_dmg.set_input_dataset('dfr3_mapping_set', mapping_set)\n",
    "\n",
    "bldg_dmg.set_parameter(\"hazard_type\", \"tsunami\")\n",
    "bldg_dmg.set_parameter(\"num_cpu\", 4)\n",
    "\n",
    "result_name = os.path.join(path_to_dmg, 'buildings_tsu_{}yr_opt0' .format(event))\n",
    "hazard_id = tsunami_hazard_dict[event]\n",
    "bldg_dmg.set_parameter(\"hazard_id\", hazard_id)\n",
    "bldg_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "bldg_dmg.run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, creating output with relocated structures (i.e. tsunami damage is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsu_data = os.path.join(path_to_dmg, 'buildings_tsu_{}yr_opt0.csv' .format(event))\n",
    "df = pd.read_csv(tsu_data)\n",
    "df['DS_0'] = 1\n",
    "df['DS_1'] = 0\n",
    "df['DS_2'] = 0\n",
    "df['DS_3'] = 0\n",
    "df['LS_0'] = 0\n",
    "df['LS_1'] = 0\n",
    "df['LS_2'] = 0\n",
    "\n",
    "path_out = os.path.join(path_to_dmg, 'buildings_tsu_{}yr_opt1.csv' .format(event))\n",
    "df.to_csv(path_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-hazard damage analysis\n",
    "\n",
    "| Mitigation option | Description | Earthquake | Tsunami |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 | Do nothing / Status Quo | Status Quo | Status Quo | \n",
    "| 1 | Retrofit structure to high code | Opt1 | Status Quo |\n",
    "| 2 | Relocate structure outside the inundation zone | Status Quo | Opt1 |\n",
    "| 3 | Decrease repair time | Status Quo | Status Quo |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_scenarios = [\n",
    "    ['buildings_eq_{}yr_opt0.csv' .format(event), 'buildings_tsu_{}yr_opt0.csv' .format(event)],\n",
    "    ['buildings_eq_{}yr_opt1.csv' .format(event), 'buildings_tsu_{}yr_opt0.csv' .format(event)],\n",
    "    ['buildings_eq_{}yr_opt0.csv' .format(event), 'buildings_tsu_{}yr_opt1.csv' .format(event)],\n",
    "    ['buildings_eq_{}yr_opt0.csv' .format(event), 'buildings_tsu_{}yr_opt0.csv' .format(event)]\n",
    "]\n",
    "\n",
    "\n",
    "cumulative_bldg_dmg = CumulativeBuildingDamage(client)\n",
    "\n",
    "for i in range(len(cumulative_scenarios)):\n",
    "    pth_to_eq = cumulative_scenarios[i][0]\n",
    "    pth_to_tsu = cumulative_scenarios[i][1]\n",
    "    pth_to_eq = os.path.join(path_to_dmg, pth_to_eq)\n",
    "    pth_to_tsu = os.path.join(path_to_dmg, pth_to_tsu)\n",
    "    \n",
    "    # --- running pyIncore\n",
    "    cumulative_bldg_dmg.set_parameter(\"num_cpu\", 1)\n",
    "\n",
    "    # loading datasets from CSV files into pyincore\n",
    "    eq_damage_dataset = Dataset.from_file(pth_to_eq, \"ergo:buildingDamageVer5\")\n",
    "    tsu_damage_dataset = Dataset.from_file(pth_to_tsu, \"ergo:buildingDamageVer5\")\n",
    "\n",
    "    cumulative_bldg_dmg.set_input_dataset(\"eq_bldg_dmg\", eq_damage_dataset)\n",
    "    cumulative_bldg_dmg.set_input_dataset(\"tsunami_bldg_dmg\", tsu_damage_dataset)\n",
    "\n",
    "    # defining path to output \n",
    "    result_name = os.path.join('buildings_cumulative_{}yr_opt{}' .format(event, i))\n",
    "    result_name = os.path.join(path_to_dmg, result_name)\n",
    "    \n",
    "    cumulative_bldg_dmg.set_parameter(\"result_name\", result_name)\n",
    "\n",
    "    # running analysis\n",
    "    cumulative_bldg_dmg.run_analysis()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing individual hazard results\n",
    "We only care about the multi-hazard case and don't wont too many files hanging around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(path_to_dmg):\n",
    "    for file in os.listdir(path_to_dmg):\n",
    "        if not \"cumulative\" in file:\n",
    "            file = os.path.join(path_to_dmg, file)\n",
    "            os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='econ_loss'></a>\n",
    "***\n",
    "### Direct economic loss computations\n",
    "Computing economic losses based on expected damage state and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists locally. Reading from local cached zip.\n",
      "Unzipped folder found in the local cache. Reading from it...\n"
     ]
    }
   ],
   "source": [
    "dmg_rto = [0.005, 0.155, 0.55, 0.90]  # from MAEVIS documentation (damage ratio)\n",
    "\n",
    "# reading in rmv values of each building\n",
    "bldg_dataset_id = \"5df40388b9219c06cf8b0c80\"\n",
    "data_service = DataService(client)\n",
    "dataset = Dataset.from_data_service(bldg_dataset_id, data_service)\n",
    "rd = dataset.get_inventory_reader()\n",
    "\n",
    "\n",
    "struct_typ, guid, rmv, year_built = [], [], [], []\n",
    "for row in rd:\n",
    "    guid.append(row['properties']['guid'])\n",
    "    rmv.append(row['properties']['rmv_improv'])\n",
    "    year_built.append(row['properties']['year_built'])\n",
    "    struct_typ.append(row['properties']['struct_typ'])\n",
    "df = pd.DataFrame({'guid': guid, 'rmv': rmv, 'year_built': year_built,'struct_typ':struct_typ})\n",
    "df.set_index('guid', inplace=True)\n",
    "\n",
    "DS_keys = ['DS_0', 'DS_1', 'DS_2', 'DS_3']\n",
    "for file in os.listdir(path_to_dmg):\n",
    "    \n",
    "    # reading in the results\n",
    "    file = os.path.join(path_to_dmg, file)\n",
    "    df_temp = pd.read_csv(file)\n",
    "    df_temp.set_index('guid', inplace=True)\n",
    "    \n",
    "    # adding real market value column to results if it's not already there\n",
    "    if not 'rmv' in list(df_temp.columns):\n",
    "        df_temp = pd.merge(df_temp, df, left_index=True, right_index=True)\n",
    "    \n",
    "    # computing expected direct economic losses if they do not exist\n",
    "    if not 'econ_loss' in list(df_temp.columns):\n",
    "        df_temp['econ_loss'] = (df_temp[DS_keys]*dmg_rto).sum(axis=1)*df_temp['rmv']      \n",
    "    \n",
    "    # computing expected direct economic losses if they do not exist\n",
    "    if not 'year_built' in list(df_temp.columns):\n",
    "        df_temp['year_built'] = df['year_built']\n",
    "        df_temp['year_built'].replace([0,1,200,2203], 1960)\n",
    "        \n",
    "    # writing back to output\n",
    "    df_temp.to_csv(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='repair'></a>\n",
    "***\n",
    "### Repair time estimates\n",
    "+ Building repair time information is taken from [HAZUS MH 2.1 (Earthquake Model)](https://www.fema.gov/sites/default/files/2020-09/fema_hazus_earthquake-model_technical-manual_2.1.pdf#page=613)  Section 15.2.4.\n",
    "+ HAZUS provides the median values for repair times for each damage state and building type.\n",
    "+ The repair time formulation follows [Kameshwar et al. (2019)](https://www.sciencedirect.com/science/article/pii/S0951832018315163?casa_token=CPiMJq2o8zAAAAAA:Hjr5X2tu2MWfcEG57JVwAMrn9QgiInDG_eoPsUAQXdZJ7VgaI3UyXVqILpD92IPTVG50R5MsaVA), in which only one set of repair time estimates are used for both earthquake and tsunami hazards and across building types.\n",
    "+ Kameshwar et al. (2019) assume that a lognormal restoration model is used for buildings and that the logarithmic dispersion is 0.5.\n",
    "+ Because Monte-Carlo simulation is not implemented for the optimization algorithm, the median values provided by HAZUS that are used to parameterize the lognormal restoration functions are converted to [mean values](https://www.itl.nist.gov/div898/handbook/apr/section1/apr164.htm)\n",
    "\n",
    "\n",
    "| Damage State | Median | Dispersion |\n",
    "| --- | --- | --- |\n",
    "| Insignificant/None | 0.5 | 0.5 |\n",
    "| Moderate | 60 | 0.5 |\n",
    "| Heavy | 360 | 0.5 |\n",
    "| Complete | 720 | 0.5 |\n",
    "\n",
    "The repair time is thus computed as <br>\n",
    "$$Repair = \\sum_{ds} P_{ds} \\mu_{r,ds}$$\n",
    "With \n",
    "+ $P_{ds}$: probability of being in damage state $ds$\n",
    "+ $\\mu_{r,ds}$: mean repair time associated with damage state $ds$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median repair times for DS: None, slight, moderate, extensive, and complete\n",
    "med = np.array([0.5, 60, 360, 720])\n",
    "beta = np.array([0.5, 0.5, 0.5, 0.5])\n",
    "DSs = ['DS_0', 'DS_1', 'DS_2', 'DS_3']\n",
    "\n",
    "# converting median/beta to mean values\n",
    "mean_repair_time = med*np.exp((beta**2)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_keys = ['DS_0', 'DS_1', 'DS_2', 'DS_3']\n",
    "cnt = 0\n",
    "for file in os.listdir(path_to_dmg):\n",
    "    \n",
    "    # if mitigation option 3, decrease repair time by half\n",
    "    if cnt == 3:\n",
    "        mean_repair_time = 0.5*mean_repair_time\n",
    "\n",
    "    # reading in the results\n",
    "    file = os.path.join(path_to_dmg, file)\n",
    "    df_temp = pd.read_csv(file)\n",
    "    df_temp.set_index('guid', inplace=True)\n",
    "    \n",
    "    # computing repair times if they do not exist\n",
    "    if not 'repair' in list(df_temp.columns):\n",
    "        df_temp['repair'] = (df_temp[DS_keys]*mean_repair_time).sum(axis=1)      \n",
    "\n",
    "    df_temp.to_csv(file)\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Interdependent Community Description - Seaside, OR\n",
    "\n",
    "Explore building inventory and social systems. Specifically look at how the building inventory connects with the housing unit inventory using the housing unit allocation.\n",
    "The housing unit allocation method will provide detail demographic characteristics for the community allocated to each structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaside, OR Housing unit inventory, IN-CORE_1bv6_SetupSeaside_FourInventories_2019-08-02_HUinventory.csv\n",
    "housing_unit_inv = \"5d543087b9219c0689b98234\"\n",
    "\n",
    "# Seaside, OR Address point inventory, IN-CORE_1bv6_SetupSeaside_FourInventories_2019-08-02_addresspointinventory.csv\n",
    "address_point_inv = \"5d542fefb9219c0689b981fb\"\n",
    "\n",
    "# Seaside, OR Building inventory, IN-CORE_1bv6_SetupSeaside_FourInventories_2019-08-02_buildinginventory.csv\n",
    "building_inv = \"5df40388b9219c06cf8b0c80\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_services = DataService(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blockid</th>\n",
       "      <th>addrptid</th>\n",
       "      <th>strctid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>huestimate</th>\n",
       "      <th>residential</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>410079511001025</td>\n",
       "      <td>41007000000020005S001001A</td>\n",
       "      <td>41007000000020005S</td>\n",
       "      <td>-123.900452</td>\n",
       "      <td>46.010494</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>410079511003005</td>\n",
       "      <td>41007000000020009S001001A</td>\n",
       "      <td>41007000000020009S</td>\n",
       "      <td>-123.932060</td>\n",
       "      <td>45.979836</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>410079507002020</td>\n",
       "      <td>41007020802001001S001001A</td>\n",
       "      <td>41007020802001001S</td>\n",
       "      <td>-123.918625</td>\n",
       "      <td>46.017567</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>410079507002040</td>\n",
       "      <td>41007020853008001S001001A</td>\n",
       "      <td>41007020853008001S</td>\n",
       "      <td>-123.913643</td>\n",
       "      <td>46.017326</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>410079507002040</td>\n",
       "      <td>41007020853008002S001001A</td>\n",
       "      <td>41007020853008002S</td>\n",
       "      <td>-123.913643</td>\n",
       "      <td>46.017326</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5983</th>\n",
       "      <td>410079511003077</td>\n",
       "      <td>41007026497001001S001001A</td>\n",
       "      <td>41007026497001001S</td>\n",
       "      <td>-123.942680</td>\n",
       "      <td>45.972363</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5984</th>\n",
       "      <td>410079511003076</td>\n",
       "      <td>41007026498001001S001001A</td>\n",
       "      <td>41007026498001001S</td>\n",
       "      <td>-123.943321</td>\n",
       "      <td>45.972286</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>410079511003002</td>\n",
       "      <td>41007026500001001S001001A</td>\n",
       "      <td>41007026500001001S</td>\n",
       "      <td>-123.941330</td>\n",
       "      <td>45.972206</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986</th>\n",
       "      <td>410079511003077</td>\n",
       "      <td>41007026501001001S001001A</td>\n",
       "      <td>41007026501001001S</td>\n",
       "      <td>-123.942200</td>\n",
       "      <td>45.972202</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5987</th>\n",
       "      <td>410079511003077</td>\n",
       "      <td>41007026505001001S001001A</td>\n",
       "      <td>41007026505001001S</td>\n",
       "      <td>-123.941589</td>\n",
       "      <td>45.972042</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3687 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              blockid                   addrptid             strctid  \\\n",
       "0     410079511001025  41007000000020005S001001A  41007000000020005S   \n",
       "1     410079511003005  41007000000020009S001001A  41007000000020009S   \n",
       "2     410079507002020  41007020802001001S001001A  41007020802001001S   \n",
       "3     410079507002040  41007020853008001S001001A  41007020853008001S   \n",
       "4     410079507002040  41007020853008002S001001A  41007020853008002S   \n",
       "...               ...                        ...                 ...   \n",
       "5983  410079511003077  41007026497001001S001001A  41007026497001001S   \n",
       "5984  410079511003076  41007026498001001S001001A  41007026498001001S   \n",
       "5985  410079511003002  41007026500001001S001001A  41007026500001001S   \n",
       "5986  410079511003077  41007026501001001S001001A  41007026501001001S   \n",
       "5987  410079511003077  41007026505001001S001001A  41007026505001001S   \n",
       "\n",
       "               x          y  huestimate  residential  \n",
       "0    -123.900452  46.010494           1            1  \n",
       "1    -123.932060  45.979836           1            1  \n",
       "2    -123.918625  46.017567           1            1  \n",
       "3    -123.913643  46.017326           1            1  \n",
       "4    -123.913643  46.017326           1            1  \n",
       "...          ...        ...         ...          ...  \n",
       "5983 -123.942680  45.972363           1            1  \n",
       "5984 -123.943321  45.972286           1            1  \n",
       "5985 -123.941330  45.972206           1            1  \n",
       "5986 -123.942200  45.972202           1            1  \n",
       "5987 -123.941589  45.972042           1            1  \n",
       "\n",
       "[3687 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population = pd.read_csv(\"Seaside_addresspointinventory.csv\")\n",
    "population = population.drop_duplicates(subset=['strctid'])\n",
    "population.to_csv(\"seaside_population.csv\")\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyincore.dataset.Dataset at 0x7faeb9cbef40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address = Dataset.from_file(\"seaside_population.csv\",data_type=\"incore:addressPoints\")\n",
    "address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Housing Unit Allocation \n",
    "https://github.com/IN-CORE/incore-docs/blob/master/notebooks/housingunitallocation.ipynb\n",
    "\n",
    "Rosenheim, Nathanael, Roberto Guidotti, Paolo Gardoni & Walter Gillis Peacock. (2019). Integration of detailed household and housing unit characteristic data with critical infrastructure for post-hazard resilience modeling. Sustainable and Resilient Infrastructure. doi.org/10.1080/23789689.2019.1681821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create housing allocation \n",
    "hua = HousingUnitAllocation(client)\n",
    "\n",
    "# Load input dataset\n",
    "hua.load_remote_input_dataset(\"housing_unit_inventory\", housing_unit_inv)\n",
    "hua.load_remote_input_dataset(\"address_point_inventory\", address_point_inv)\n",
    "#hua.set_input_dataset(\"address_point_inventory\", address)\n",
    "hua.load_remote_input_dataset(\"buildings\", building_inv)\n",
    "\n",
    "# Specify the result name\n",
    "result_name = \"IN-CORE_1bv6_housingunitallocation\"\n",
    "\n",
    "seed = 1238\n",
    "iterations = 1\n",
    "\n",
    "# Set analysis parameters\n",
    "hua.set_parameter(\"result_name\", result_name)\n",
    "hua.set_parameter(\"seed\", seed)\n",
    "hua.set_parameter(\"iterations\", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Housing unit allocation analysis\n",
    "hua.run_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve result dataset\n",
    "hua_result = hua.get_output_dataset(\"result\")\n",
    "\n",
    "# Convert dataset to Pandas DataFrame\n",
    "hua_df = hua_result.get_dataframe_from_csv(low_memory=False)\n",
    "\n",
    "# Display top 5 rows of output data\n",
    "hua_df[['guid','numprec','incomegroup']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hua_df = pd.read_csv(\"IN-CORE_1bv6_housingunitallocation_1238.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Found Issue - to be fixed\n",
    "The building inventory for Seaside has damage state information - a new version of the file needs to be made without the damage data.\n",
    "\n",
    "### Seaside, OR Building inventory, IN-CORE_1bv6_SetupSeaside_FourInventories_2019-08-02_buildinginventory.csv\n",
    "building_inv = \"5d5433edb9219c0689b98344\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hua_df = hua_df.drop(columns= ['insignific','moderate','heavy','complete'])\n",
    "hua_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hua_df = hua_df.loc[hua_df['aphumerge'] == 'both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hua_df['Race Ethnicity'] = \"0 Vacant HU No Race Ethnicity Data\"\n",
    "hua_df['Race Ethnicity'].notes = \"Identify Race and Ethnicity Housing Unit Characteristics.\"\n",
    "\n",
    "hua_df.loc[(hua_df['race'] == 1) & (hua_df['hispan'] == 0),'Race Ethnicity'] = \"1 White alone, Not Hispanic\"\n",
    "hua_df.loc[(hua_df['race'] == 2) & (hua_df['hispan'] == 0),'Race Ethnicity'] = \"2 Black alone, Not Hispanic\"\n",
    "hua_df.loc[(hua_df['race'].isin([3,4,5,6,7])) & (hua_df['hispan'] == 0),'Race Ethnicity'] = \"3 Other Race, Not Hispanic\"\n",
    "hua_df.loc[(hua_df['hispan'] == 1),'Race Ethnicity'] = \"4 Any Race, Hispanic\"\n",
    "hua_df.loc[(hua_df['gqtype'] >= 1),'Race Ethnicity'] = \"5 Group Quarters no Race Ethnicity Data\"\n",
    "\n",
    "# Check new variable\n",
    "table_title = \"Confirm housing unit characteristic by Race and Ethnicity.\"\n",
    "pd.crosstab(hua_df['Race Ethnicity'], hua_df['race'], \n",
    "            margins=True, margins_name=\"Total\").style.set_caption(table_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check new variable\n",
    "table_title = \"Confirm housing unit characteristic by Race and Ethnicity.\"\n",
    "pd.crosstab(hua_df['Race Ethnicity'], hua_df['hispan'], \n",
    "            margins=True, margins_name=\"Total\").style.set_caption(table_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_title = \"Table 1. Housing Unit Characteristics by Race and Ethnicity\"\n",
    "table1 = pd.pivot_table(hua_df, values='numprec', index=['Race Ethnicity'],\n",
    "                              margins = True, margins_name = 'Total',\n",
    "                              aggfunc=[len, np.sum], \n",
    "                              fill_value=0).reset_index().rename(\n",
    "                                                            columns={'len': 'Housing Unit',\n",
    "                                                                     'sum' : 'Population',\n",
    "                                                                     'numprec': 'Count'})\n",
    "\n",
    "varformat = {('Housing Unit','Count'): \"{:,}\", ('Population','Count'): \"{:,}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1.style.set_caption(table_title).format(varformat).set_table_styles([\n",
    "    dict(selector='th', props=[('text-align', 'center')]),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Housing Unit Allocation has worked\n",
    "Notice that the population count totals for the community should match (pretty closely) data collected for the 2010 Decennial Census.\n",
    "This can be confirmed by going to data.census.gov\n",
    "\n",
    "https://data.census.gov/cedsci/table?q=DECENNIALPL2010.P1&g=1600000US4165950&tid=DECENNIALSF12010.P1\n",
    "\n",
    "Differences in the housing unit allocation and the Census count may be due to differences between political boundaries and the building inventory. See Rosenheim et al 2019 for more details.\n",
    "\n",
    "The housing unit allocation results will become the input for the dislocation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned HUA file as CSV\n",
    "hua_df.to_csv('IN-CORE_1cv1_housingunitallocation_1238.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create population dislocatin \n",
    "pop_dis = PopulationDislocation(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaside, OR Housing unit allocation, performed at start of notebook\n",
    "housing_unit_alloc = Dataset.from_file('IN-CORE_1cv1_housingunitallocation_1238.csv','incore:housingUnitAllocation')\n",
    "\n",
    "# Seaside, OR \"IN-CORE_1bv6_SetupSeaside_FourInventories_2019-08-02_bgdata.csv\"\n",
    "bg_data = \"5d542bd8b9219c0689b90408\"\n",
    "\n",
    "# Value loss parameters, \"IN-CORE_value_loss_bai09.csv\"\n",
    "value_loss = \"5dfd1069fc33d500081555d8\"\n",
    "\n",
    "# Load input dataset\n",
    "pop_dis.set_input_dataset(\"housing_unit_allocation\", housing_unit_alloc)\n",
    "pop_dis.load_remote_input_dataset(\"block_group_data\", bg_data)\n",
    "pop_dis.load_remote_input_dataset(\"value_poss_param\", value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create population dislocatin \n",
    "pop_dis = PopulationDislocation(client)\n",
    "\n",
    "# Load input dataset\n",
    "bldg_dmg_result = Dataset.from_file(\"buildings_cumulative_500yr_opt0.csv\", data_type='ergo:buildingDamageVer5')\n",
    "pop_dis.set_input_dataset(\"building_dmg\", bldg_dmg_result)\n",
    "pop_dis.set_input_dataset(\"housing_unit_allocation\", housing_unit_alloc)\n",
    "pop_dis.load_remote_input_dataset(\"block_group_data\", bg_data)\n",
    "pop_dis.load_remote_input_dataset(\"value_poss_param\", value_loss)\n",
    "\n",
    "# Specify the result name\n",
    "result_name = \"IN-CORE_1bv6_population_dislocation\"\n",
    "\n",
    "seed = 1111\n",
    "\n",
    "# Set analysis parameters\n",
    "pop_dis.set_parameter(\"result_name\", result_name)\n",
    "pop_dis.set_parameter(\"seed\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dis.run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='disloc'></a>\n",
    "***\n",
    "### Population dislocation estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dis = PopulationDislocation(client)\n",
    "seed = 1111\n",
    "cnt = 0\n",
    "for file in os.listdir(path_to_dmg):\n",
    "\n",
    "    #Reading in local building damage files\n",
    "    file = os.path.join(path_to_dmg, file)\n",
    "    dmg_data = Dataset.from_file(file, data_type='ergo:buildingDamageVer5')\n",
    "    pop_dis.set_input_dataset(\"building_dmg\", dmg_data)\n",
    "\n",
    "    # loading remote population dislocation data \n",
    "    pop_dis.load_remote_input_dataset(\"housing_unit_allocation\", \"5d543b06b9219c0689b987af\")\n",
    "    pop_dis.load_remote_input_dataset(\"block_group_data\", \"5d542bd8b9219c0689b90408\")\n",
    "    pop_dis.load_remote_input_dataset(\"value_poss_param\", \"5dfd1069fc33d500081555d8\") \n",
    "\n",
    "    # setting population dislocation run information\n",
    "    result_name = \"housing-dislocation-result-opt{}\".format(cnt)\n",
    "    result_name = os.path.join(path_to_dislocation, result_name)\n",
    "    pop_dis.set_parameter(\"result_name\", result_name)\n",
    "    pop_dis.set_parameter(\"seed\", seed)\n",
    "    \n",
    "    # running population dislocation\n",
    "    pop_dis.run_analysis()\n",
    "    \n",
    "    # reading \n",
    "    df = pd.read_csv('{}.csv' .format(result_name))\n",
    "    df.dislocated = df.dislocated.astype(\"int32\")\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.drop_duplicates(subset=['guid'])\n",
    "    df.to_csv(result_name+\".csv\", index=False)\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmg_files = os.listdir(path_to_dmg)\n",
    "dslc_files = os.listdir(path_to_dislocation)\n",
    "\n",
    "for i in range(4):\n",
    "    dmg_file = os.path.join(path_to_dmg, dmg_files[i])\n",
    "    dmg_df = pd.read_csv(dmg_file)\n",
    "    dmg_df.set_index('guid', inplace=True)\n",
    "\n",
    "    if not 'dislocated' in list(dmg_df.columns):\n",
    "        dslc_file = os.path.join(path_to_dislocation, dslc_files[i])\n",
    "        dslc_df = pd.read_csv(dslc_file)\n",
    "        dslc_df.set_index('guid', inplace=True)    \n",
    "        dmg_df = pd.merge(dmg_df, dslc_df['dislocated'], how='left', left_index=True, right_index=True)\n",
    "\n",
    "        dmg_df.to_csv(dmg_file, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing population dislocation results\n",
    "This data has been compiled into the damage anlaysis files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(path_to_dislocation):\n",
    "    for file in os.listdir(path_to_dislocation):\n",
    "        file = os.path.join(path_to_dislocation, file)\n",
    "        os.remove(file)\n",
    "    os.rmdir(path_to_dislocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "<a id='optimization'></a>\n",
    "<h2><center> Optimization </center><h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='agg-opt'></a>\n",
    "***\n",
    "### Data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level0 = pd.read_csv(\"damage_results/buildings_cumulative_{}yr_opt0.csv\".format(event))\n",
    "level0[\"K\"]= 0\n",
    "level0[\"b\"]=1\n",
    "level1 = pd.read_csv(\"damage_results/buildings_cumulative_{}yr_opt1.csv\".format(event))\n",
    "level1[\"K\"]= 1\n",
    "level1[\"b\"]=0\n",
    "level1[\"dislocated\"] = level1.dislocated*0.80\n",
    "level2 = pd.read_csv(\"damage_results/buildings_cumulative_{}yr_opt2.csv\".format(event))\n",
    "level2[\"K\"]= 2\n",
    "level2[\"b\"]=0\n",
    "level2[\"dislocated\"] = level1.dislocated*0.50\n",
    "level3 = pd.read_csv(\"damage_results/buildings_cumulative_{}yr_opt3.csv\".format(event))\n",
    "level3[\"K\"]= 3\n",
    "level3[\"b\"]=0\n",
    "level3[\"dislocated\"] = level1.dislocated*1\n",
    "level0.fillna(0, inplace=True)\n",
    "level1.fillna(0, inplace=True)\n",
    "level2.fillna(0, inplace=True)\n",
    "level3.fillna(0, inplace=True)\n",
    "print(level1[['guid', 'dislocated']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_all_lvls = pd.concat([level0, level1, level2, level3])\n",
    "ss_all_lvls = ss_all_lvls.groupby([\"guid\", \"struct_typ\", \"K\"]).mean().reset_index()\n",
    "ss_all_lvls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='optimization'></a>\n",
    "***\n",
    "### Optimization Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qt_data = ss_all_lvls.copy()\n",
    "qt_data.rename(columns = {'guid':'Z'}, inplace = True)\n",
    "qt_data.rename(columns = {'struct_typ':'S'}, inplace = True)\n",
    "qt_data.rename(columns = {'dislocated':'d_ijk'}, inplace = True)\n",
    "qt_data.rename(columns = {'econ_loss':'l'}, inplace = True)\n",
    "qt_data[\"Q_t_hat\"] = ss_all_lvls.repair/ss_all_lvls.b.sum()\n",
    "qt_data = qt_data.drop(['LS_0', 'LS_1', 'LS_2', 'DS_0','DS_1', 'DS_2', 'DS_3','year_built'], axis=1)\n",
    "qt_data.to_csv(\"seaside_qt_data_{}.csv\".format(event))\n",
    "qt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Buildings:\",qt_data.b.sum())\n",
    "print(\"Repair time:\",np.sum(qt_data.b*qt_data.Q_t_hat))\n",
    "print(\"Buildings Dislocated:\",np.sum(qt_data.b*qt_data.d_ijk))\n",
    "print(\"Economic Loss:\",np.sum(qt_data.b*qt_data.l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level0['k1cost'] = level0.rmv*0.20  #30% of rmv for stategy 1\n",
    "level0['k2cost'] = level0.rmv*1  #100% of rmv for stategy 2\n",
    "level0['k3cost'] = level0.rmv*0.10  #15% of rmv for stategy 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Sc(df_lQaddedb,df):\n",
    "    df_Sc=df_lQaddedb[:]\n",
    "    df_Sc=df_Sc.loc[df_Sc.index.repeat(4)].reset_index(drop=True)\n",
    "    df_Sc[\"K'\"]=[0,1,2,3]*len(df_lQaddedb)\n",
    "    df_Sc.drop([\"b\",\"Q_t_hat\"],axis=1,inplace=True)\n",
    "    df_Sc[\"Sc\"]=0\n",
    "    for index,row in df_Sc.iterrows():\n",
    "        r=df[(df[\"guid\"]==row[\"Z\"])&(df[\"struct_typ\"]==row[\"S\"])]\n",
    "        if row[\"K'\"]<row[\"K\"]:\n",
    "            df_Sc.loc[index,\"Sc\"]=100000000000000\n",
    "        if row[\"K\"]==0:\n",
    "            if row[\"K'\"]==1:\n",
    "                df_Sc.loc[index,\"Sc\"]=r[\"k1cost\"].item()\n",
    "            elif row[\"K'\"]==2:\n",
    "                df_Sc.loc[index,\"Sc\"]=r[\"k2cost\"].item()\n",
    "            elif row[\"K'\"]==3:\n",
    "                df_Sc.loc[index,\"Sc\"]=r[\"k3cost\"].item()\n",
    "#         elif row[\"K\"]==1:\n",
    "#             if row[\"K'\"]==2:\n",
    "#                 df_Sc.loc[index,\"Sc\"]=r[\"k2cost\"].item()\n",
    "#             elif row[\"K'\"]==3:\n",
    "#                 df_Sc.loc[index,\"Sc\"]=r[\"k3cost\"].item()\n",
    "#         elif row[\"K\"]==2:\n",
    "#             if row[\"K'\"]==3:\n",
    "#                 df_Sc.loc[index,\"Sc\"]=r[\"k3cost\"].item()\n",
    "    df_Sc.drop( df_Sc[ df_Sc[\"K'\"]<df_Sc[\"K\"] ].index , inplace=True)\n",
    "    df_Sc = df_Sc[df_Sc[\"K\"]==0]\n",
    "    return df_Sc\n",
    "\n",
    "df_sc = calculate_Sc(qt_data, level0)\n",
    "df_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc.to_csv(\"seaside_sc_data_{}.csv\".format(event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plotting'></a>\n",
    "***\n",
    "<h2><center> Plotting Results </center><h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in building polygon dataset and setting up function to filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in building polygon dataset\n",
    "bldg_dataset_id = \"5d927ab2b9219c06ae8d313c\"\n",
    "data_service = DataService(client)\n",
    "dataset = Dataset.from_data_service(bldg_dataset_id, data_service)\n",
    "rd = dataset.get_inventory_reader()\n",
    "\n",
    "# setting up geodataframe\n",
    "gdf = gpd.GeoDataFrame.from_features([feature for feature in rd], crs=\"EPSG:3857\")\n",
    "gdf = gdf[['guid', 'geometry']]\n",
    "gdf.dropna(subset=['guid'], inplace=True)\n",
    "gdf.drop_duplicates(subset='guid', inplace=True)\n",
    "gdf.set_index('guid', inplace=True)\n",
    "\n",
    "def get_gdf_feats(gdf, path_to_run=None, values=None, k=None):\n",
    "    df = pd.read_csv(path_to_run)\n",
    "    df.set_index('Z', inplace=True)\n",
    "    if values != None:\n",
    "        df = df.loc[df['Values'].isin(values)]\n",
    "    if k != None:\n",
    "        df = df.loc[df['K'].isin(k)]\n",
    "    gdf_new = pd.merge(gdf, df, left_index=True, right_index=True)\n",
    "    return gdf_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawing map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- making figure and map\n",
    "fig, ax = plt.subplots(1,1)\n",
    "gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# plotting all parcels\n",
    "gdf.plot(ax=ax, color='lightblue')\n",
    "\n",
    "# getting a subset of parcels\n",
    "gdf_k0 = get_gdf_feats(gdf, path_to_run='decision_variable_B60_X369.csv', values=[1,2], k=[0])\n",
    "\n",
    "# drawing the subset of parcels on the map\n",
    "gdf_k0.plot(ax=ax, color='salmon')\n",
    "\n",
    "\n",
    "# adding basemap\n",
    "ctx.add_basemap(ax, zoom=15, crs='EPSG:4326', source=ctx.providers.Stamen.TonerLite)\n",
    "# ctx.add_basemap(ax, zoom=15, crs='EPSG:4326', source=ctx.providers.OpenStreetMap.Mapnik)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
